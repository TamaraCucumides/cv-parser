{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import es_core_news_sm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import es_core_news_sm\n",
    "import itertools\n",
    "from nltk.stem import SnowballStemmer\n",
    "import textacy\n",
    "import regex\n",
    "import json\n",
    "import unidecode\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import yaml\n",
    "wordvectors_file_vec ='/home/erwin/Genoma/cv-parser/parser/embeddings/fasttext-sbwc.3.6.e20.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantidad = 300000\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_json = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_seccionado'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "#print(json_files)  # for me this prints ['foo.json']\n",
    "jsons = []\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js)) as json_file:\n",
    "        jsons.append(json.load(json_file))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marketing gestión comercial persona proactiva ordenar alto sentir responsabilidad equipar fundamental empresa orientar resultar tangible experiencia laboral adecco peru cargo gerente desarrollo fecha 01/04/2019 fecho funciones responsable manejar cartero top cliente responsable cumplimiento objetivo venta líneo negociar garantizar rentabilidad nivel servicio desarrollo convenio diverso institución implementación canal post-venta cliente extranjero enfoque negociar cliente extranjero desarrollar servicio medir cliente adecco peru cargo gerente sucursal fecha 01/01/2017 31/03/2019 funciones responsable gestión integral sucursal responsable cumplimiento objetivo venta líneo negociar garantizar rentabilidad nivel servicio desarrollo ejecución plan negocio venta kpis estrategia comercial orientar captación mantenimiento retención cliente cargo ejecutivo comercial fecha 25/03/2015 31/12/2016 funciones ejecutar estrategia comercial orientar captación mantenimiento retención cliente sucursal identificar información negocio relevante oportunidad comercial seguimiento propuesta comercial presentar cliente mantener comunicación estable periódico cliente realizar visitar mantenimiento seguimiento gestión facturación cobranza cliente elaborar propuesta comercial acorde servicio logros premio latam gestión comercial 2015 freno s.a cargo coordinador inteligencia comercial fecha 01/08/2014 20/03/2015 funciones supervisión fuerza ventas lima estudio mercar lanzamiento producto mejorar portafolio producto servicio post-venta distribuidor minorista prospección punto venta lima metropolitana bolsa valores lima cargo ejecutivo comercial fecha 01/12/2012 31/07/2014 ejecutar estrategia comercial orientar captación mantenimiento retención cliente identificar información negocio relevante oportunidad comercial elaborar propuesta comercial acorde servicio logros meta anual 2013 superar 30 idiomas ingles nivel avanzado '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsons[0]['experiencia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    '''Generate Vectora for sentences.'''\n",
    "    M = []\n",
    "    for w in s.split():\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v/np.sqrt((v**2).sum())\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    '''Return Cosine Similarity.'''\n",
    "    return  np.dot(vec1,vec2)/(np.linalg.norm(vec1)* np.linalg.norm(vec2))\n",
    "\n",
    "def get_closest(word, n):\n",
    "    '''Get n most similar words by words.'''\n",
    "    #This function can easily be expanded to get similar words to phrases--\n",
    "    #using sent2vec() method defined in WithWord2Vec notebook. \n",
    "    word = word.lower()\n",
    "    words = [word]\n",
    "    similar_vals = [1]\n",
    "    try:\n",
    "        similar_list = model.most_similar(positive=[word],topn=n)\n",
    "        \n",
    "        for tupl in similar_list:\n",
    "            words.append(tupl[0])\n",
    "            similar_vals.append(tupl[1])\n",
    "    except:\n",
    "        #If word not in vocabulary return same word and 1 similarity-- \n",
    "        #see initialisation of words, similarities.\n",
    "        pass\n",
    "    \n",
    "    return words, similar_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3823823\n"
     ]
    }
   ],
   "source": [
    "frase_1 = 'corredor de bolsa'\n",
    "frase_2 = 'ingeniero comercial'\n",
    "\n",
    "\n",
    "vector_sentence_1 = sent2vec(frase_1)\n",
    "vector_sentence_2 = sent2vec(frase_2)\n",
    "similitud = cosine_sim(vector_sentence_1,vector_sentence_2)\n",
    "\n",
    "print(similitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('microsoft', 0.4660836458206177), ('xcode', 0.45749515295028687), ('barcode', 0.4524089992046356), ('tbarcode', 0.4520794153213501), ('oracle', 0.4513850212097168), ('quickbooks', 0.45035749673843384), ('koffice', 0.4440692663192749), ('kingsoft', 0.43898719549179077), ('iwork', 0.4389722943305969), ('delphi', 0.4341479241847992)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['excel', 'sap'], negative=['datos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('corredores', 0.6894433498382568),\n",
       " ('excorredor', 0.6477311849594116),\n",
       " ('corredoria', 0.5702216029167175),\n",
       " ('corredora', 0.5340113043785095),\n",
       " ('esprinter', 0.5244001150131226),\n",
       " ('velocista', 0.5186375975608826),\n",
       " ('ciclista', 0.5133304595947266),\n",
       " ('pasillo', 0.5123224258422852),\n",
       " ('corredoras', 0.4917184114456177),\n",
       " ('pedalista', 0.4917040169239044),\n",
       " ('esprint', 0.4876434803009033),\n",
       " ('esprinters', 0.4859365224838257),\n",
       " ('bioceánico', 0.4820821285247803),\n",
       " ('ciclistas', 0.4741329252719879),\n",
       " ('corridor', 0.47213947772979736),\n",
       " ('túnel', 0.4685114622116089),\n",
       " ('viario', 0.4659390449523926),\n",
       " ('tramo', 0.4606683850288391),\n",
       " ('trazado', 0.4597742259502411),\n",
       " ('contrarrelojista', 0.4591130018234253)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word('corredor', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ingle', 'pantorrilla', 'pierna', 'rodilla'],\n",
       " [1, 0.733283281326294, 0.7193220853805542, 0.7045726776123047])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest(word= 'ingle', n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6669351"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('tradición', 'tradicional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'educar'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import es_core_news_md\n",
    "nlp = es_core_news_md.load()\n",
    "nlp_text = nlp('educado')\n",
    "nlp_text[0].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_description = '''ingeniería máster postgrado excel desarrollo gestión comercial experiencia manejo clientes\n",
    "emprendimiento liderar equipos planificar organizar dirigir trabajo presión seguimiento KPI inglés '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/prateekguptaiiitk/Resume_Filtering/blob/develop/Scoring/CV_ranking.ipynb\n",
    "word_value = {}\n",
    "similar_words_needed = 1\n",
    "for word in prc_description.split():\n",
    "    similar_words, similarity = get_closest(word, similar_words_needed)\n",
    "    for i in range(len(similar_words)):\n",
    "        word_value[similar_words[i]] = word_value.get(similar_words[i], 0)+similarity[i]\n",
    "        #print(similar_words[i], word_value[similar_words[i]])\n",
    "        #print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ingeniería', 'ingenierías', 'máster', 'másteres', 'postgrado', 'postgrados', 'excel', 'powerpoint', 'desarrollo', 'sostenible', 'gestión', 'gestionar', 'comercial', 'comerciales', 'experiencia', 'experiencial', 'manejo', 'manejar', 'clientes', 'cliente', 'emprendimiento', 'emprendimientos', 'liderar', 'encabezar', 'equipos', 'quipos', 'planificar', 'planear', 'organizar', 'organizarla', 'dirigir', 'dirigirlo', 'trabajo', 'remunerado', 'presión', 'presiones', 'seguimiento', 'seguimientos', 'kpi', 'krt', 'inglés', 'ingles'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_value.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### ahora veamos si resulta el ranking\n",
    "#frecuencia de término – frecuencia inversa de documento \n",
    "#Tf-idf\n",
    "#Para calcular este ranking es mejor tener las secciones skills y experiencia\n",
    "#con el fin de calcular esta metrica usando la ocurrencia de las palabras\n",
    "# Tenemos todos los CV's y una descripción del cargo, a este descripcion del cargo\n",
    "# tiene N palabras, le buscamos 2 palabras parecidas, generando una descripcion\n",
    "# de N*2\n",
    "\n",
    "# Usando esta nuevo set de palabras de descripcion, recorremos todos los cvs contando \n",
    "# la ocurrencia de estas palabras en cada documento, y luego se genera un ranking usando Tf-idf\n",
    "# La pregunta es: ¿Lo haré sobre el documento entero? o ¿Trataré de seccionar y ocupar ciertas secciones?\n",
    "\n",
    "no_of_cv = len(jsons)\n",
    "#print(no_of_cv)\n",
    "\n",
    "count = {}\n",
    "idf = {}\n",
    "for word in word_value.keys():\n",
    "    count[word] = 0\n",
    "    for i in range(no_of_cv):\n",
    "        try:\n",
    "            if word in jsons[i]['skills'] or word in jsons[i]['experiencia'] or word in jsons[i]['educación']:\n",
    "                #print('entre')\n",
    "                count[word] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    idf[word] = math.log((no_of_cv+1)/(1+count[word]))\n",
    "    #print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculemos usando similitud\n",
    "\n",
    "\n",
    "\n",
    "def calculo_similitud(word1, array_palabras):\n",
    "    n_veces = 0\n",
    "    for word in array_palabras:\n",
    "        try:\n",
    "            sim = model.similarity(word, word1)\n",
    "            #print(word)\n",
    "            #print(sim)\n",
    "            if sim > 0.5:\n",
    "                n_veces += 1\n",
    "            else:\n",
    "                continue\n",
    "        except: #No estaba la palabra\n",
    "            pass\n",
    "        \n",
    "    return n_veces\n",
    "\n",
    "\n",
    "\n",
    "word1 = 'feliz'\n",
    "array = ['alegre', 'contento', 'taladro', 'juguete']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n = calculo_similitud(word1, array)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsons[0]['skills'].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/erwin/Genoma/cv-parser/parser/diccionarios/stop_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-203c73b513da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_of_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# eliminación de stopwords y quizas lematizacion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mskill_pro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'skills'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mexpe_pro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'experiencia'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0medu_pro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'educación'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-92a080c851f5>\u001b[0m in \u001b[0;36mpre_process\u001b[0;34m(corpus, enminiscula)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mEsta\u001b[0m \u001b[0mpropiedad\u001b[0m \u001b[0mde\u001b[0m \u001b[0mmantener\u001b[0m \u001b[0mla\u001b[0m \u001b[0mcapitalización\u001b[0m \u001b[0mes\u001b[0m \u001b[0mútil\u001b[0m \u001b[0men\u001b[0m \u001b[0mla\u001b[0m \u001b[0mdetección\u001b[0m \u001b[0mde\u001b[0m \u001b[0mnombres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     '''\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mnewStopWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcargar_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/erwin/Genoma/cv-parser/parser/diccionarios/stop_words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spanish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewStopWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-16c0f261ac3f>\u001b[0m in \u001b[0;36mcargar_dict\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mUtilidad\u001b[0m \u001b[0mpara\u001b[0m \u001b[0mcargar\u001b[0m \u001b[0mlos\u001b[0m \u001b[0mdiccionarios\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     '''\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# '' aparece cuando hay lines vacias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/erwin/Genoma/cv-parser/parser/diccionarios/stop_words'"
     ]
    }
   ],
   "source": [
    "no_of_cv = len(jsons)\n",
    "\n",
    "count = {}\n",
    "idf = {}\n",
    "for word in word_value.keys():\n",
    "    count[word] = 0\n",
    "    for i in range(no_of_cv):\n",
    "        # eliminación de stopwords y quizas lematizacion\n",
    "        skill_pro = pre_process(jsons[i]['skills']) \n",
    "        expe_pro = pre_process(jsons[i]['experiencia'])\n",
    "        edu_pro = pre_process(jsons[i]['educación'])\n",
    "        \n",
    "        if calculo_similitud(word, skill_pro.split()) or calculo_similitud(word, expe_pro.split()) or calculo_similitud(word, edu_pro.split()):\n",
    "            count[word] += 1\n",
    "\n",
    "\n",
    "    idf[word] = math.log((no_of_cv+1)/(1+count[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score = {}\n",
    "for i in range(no_of_cv):\n",
    "    score[i] = 0\n",
    "    skill_pro = pre_process(jsons[i]['skills']) \n",
    "    expe_pro = pre_process(jsons[i]['experiencia'])\n",
    "    edu_pro = pre_process(jsons[i]['educación'])\n",
    "    for word in word_value.keys():\n",
    "        \n",
    "        \n",
    "        \n",
    "        tf = 1 + calculo_similitud(word, skill_pro.split()) + calculo_similitud(word, expe_pro.split()) + calculo_similitud(word, edu_pro.split())\n",
    "\n",
    "        score[i] += word_value[word]*tf*idf[word]\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = []\n",
    "for i in range(no_of_cv):\n",
    "    sorted_list.append((score[i], jsons[i]['nombre archivo']))\n",
    "    \n",
    "sorted_list.sort(reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "import spacy\n",
    "import es_core_news_sm\n",
    "\n",
    "\n",
    "def pre_process(corpus):\n",
    "    corpus = corpus.lower()\n",
    "\n",
    "    stopset = stopwords.words('spanish') + list(string.punctuation)\n",
    "\n",
    "    corpus = \" \".join([i for i in word_tokenize(corpus) if i not in stopset])\n",
    "    # remove non-ascii characters\n",
    "    corpus = unidecode(corpus)\n",
    "    return corpus\n",
    "\n",
    "def lematizar(frase):\n",
    "    nlp = es_core_news_sm.load()\n",
    "    doc = nlp(frase)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    return lemmas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pre_process('educación básica')\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = lematizar('ingeniero')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lematizar('ingeniero') in lematizar('ingenieria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('ingeniero', 'ingenieria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "   # stemmed_claves = [stemmer.stem(token) for token in palabras_claves]\n",
    "#stop_words = set(stopwords.words('spanish')) \n",
    "\n",
    "[stemmer.stem(x) for x in test] \n",
    "#stemmed_clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('ingeniería') in stemmer.stem('ingeniería')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def cargar_dict(path):\n",
    "    with open(path) as f:  \n",
    "        array = [x.strip() for x in f]\n",
    "        c = [x for x in array if x != ''] # '' aparece cuando hay lines vacias\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newStopWords = cargar_dict('/home/erwin/Genoma/cv-parser/parser/diccionarios/stop_words')\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "stopwords.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def pre_process(corpus, stopWords, enminiscula = True):\n",
    "    if enminiscula:\n",
    "        corpus = corpus.lower()\n",
    "    stopset = stopwords+ list(string.punctuation)\n",
    "\n",
    "    corpus = \" \".join([i for i in word_tokenize(corpus) if i not in stopset])\n",
    "    # remove non-ascii characters\n",
    "    #corpus = unidecode.unidecode(corpus)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process('UNIVERSIDAD Universidad universidad', stopwords, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import es_core_news_sm\n",
    "def lematizar(frase):\n",
    "    '''\n",
    "    Esta función recibe un string y le aplica lematización:\n",
    "    ingeniero ---> ingenier\n",
    "    ingeniera ----> ingenier\n",
    "    '''\n",
    "    nlp = es_core_news_sm.load()\n",
    "    doc = nlp(frase)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = '''\n",
    "Nos encontramos en la búsqueda de Socios comerciales, para administrar por completo importantes puntos de ventas (corner) de retail de calzado.\n",
    "\n",
    "Has escuchado hablar de las franquicias y consignaciones? pues esta empresa trabaja a través de la consignación, esto quiere decir que la empresa te hace entrega del punto de venta, corriendo con todos los gastos de infraestructura, arriendo y toda la mercadería y renovación de stock constante, apoyo de visual y marketing,etc.\n",
    "Buscamos vendedores que se encarguen de la administración por completo del punto de venta (RRHH,seguimiento y gestión inventarios, bodegas, ventas, etc.). Ofrecemos atractivas comisiones\n",
    "\n",
    "Los montos líquidos PROMEDIO mensual a los que puedes acceder, posterior a los gastos de administración van desde: $600.000 a $1.000.000\n",
    "\n",
    "Requisitos:\n",
    "-Experiencia en ventas (dependiente o independiente)\n",
    "-Disponibilidad para horarios de mall\n",
    "-Motivación y orientación al logro y a la venta\n",
    "-Ambición y competitividad\n",
    "-Manejo conceptos básicos de contabilidad y legislación laboral\n",
    "-Excelente capacidad de administrar un equipo.\n",
    "-Posee una sociedad limitada (si no cuenta con esto, no es impedimento, puesto que es tramite simple que se realiza de manera on line)\n",
    "\n",
    "\n",
    "Buscamos personas que deseen emprender, tengan orientación a la venta y el liderazgo, para todos ellos esta es la oportunidad para iniciar un negocio propio con el respaldo de una marca líder en su rubro.\n",
    "'''\n",
    "\n",
    "\n",
    "f =lematizar(des)\n",
    "lematizado = ''\n",
    "for word in f:\n",
    "    lematizado += word +' '\n",
    "lematizado.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "def pre_process(corpus,  enminiscula= True):\n",
    "    '''\n",
    "    Entrada: texto, stopwords, enminiscula (opcional)\n",
    "    Salida:  texto\n",
    "    Funcion que se encarga de limpiar las stopwords de un texto\n",
    "    el parámetro opciona enminuscula si es verdadero,\n",
    "    transforma todo el texto a miniscula y elimina stopwords que esten en minuscula.\n",
    "    Cuando se usa false, el texto retornado mantendra capitalizacion original y \n",
    "    además se eliminan stop words especificas tales como: Pontificia, Universidad, Vitae, VITAE\n",
    "    Notar que stop_words.txt tiene stopwords en minisculas y capitalizada.\n",
    "    Esta propiedad de mantener la capitalización es útil en la detección de nombres.\n",
    "    '''\n",
    "    newStopWords = cargar_dict('/home/erwin/Genoma/cv-parser/parser/diccionarios/stop_words')\n",
    "    stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "    print(newStopWords)\n",
    "    stop = stopwords.extend(newStopWords)\n",
    "    \n",
    "\n",
    "    if enminiscula:\n",
    "        corpus = corpus.lower()\n",
    "    stopset = stopwords+ list(string.punctuation)\n",
    "\n",
    "    corpus = \" \".join([i for i in word_tokenize(corpus) if i not in stopset])\n",
    "    # remove non-ascii characters\n",
    "    #corpus = unidecode.unidecode(corpus)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_dict(path):\n",
    "    '''\n",
    "    Utilidad para cargar los diccionarios\n",
    "    '''\n",
    "    with open(path) as f:  \n",
    "        array = [x.strip() for x in f]\n",
    "        c = [x for x in array if x != ''] # '' aparece cuando hay lines vacias\n",
    "    return c\n",
    "\n",
    "file = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1586903416737-NewHTJ'\n",
    "cv_txt = cargar_dict(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modificar(word):\n",
    "    '''\n",
    "     Retornar el lema de la palabra. \n",
    "   Fijandose que no sean un simbolo raro ni una stopword.\n",
    "    '''\n",
    "    try:\n",
    "        symbols = '''~`!@#$%^&*)(_+-=}{][|\\:;\",./<>?'''\n",
    "        mod_word = ''\n",
    "        \n",
    "        for char in word:\n",
    "            if (char not in symbols):\n",
    "                mod_word += char.lower()\n",
    "\n",
    "        docx = nlp(mod_word)\n",
    "\n",
    "        if (len(mod_word) == 0 or docx[0].is_stop):\n",
    "            return None\n",
    "        else:\n",
    "            return docx[0].lemma_\n",
    "    except:\n",
    "        return None # to handle the odd case of characters like 'x02', etc.\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def esta_vacia(line):\n",
    "    '''\n",
    "    Retorna un booleano correspondiendo a \n",
    "    si una linea esta vacia en términos de letras-números\n",
    "    '''\n",
    "    for c in line:\n",
    "        if (c.isalpha()):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \" \".join(cv_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secciones_limpio(dataframe):\n",
    "    ar = [str(exp).lower() for exp in dataframe if str(exp)!='nan' and str(exp)!= ' ']\n",
    "    return ar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Perfil</th>\n",
       "      <th>Experiencia</th>\n",
       "      <th>Educacion</th>\n",
       "      <th>Cursos</th>\n",
       "      <th>Habilidades</th>\n",
       "      <th>Contacto</th>\n",
       "      <th>Referencias</th>\n",
       "      <th>Logros</th>\n",
       "      <th>Hobbies</th>\n",
       "      <th>Emprendimientos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Perfil</td>\n",
       "      <td>Experiencia Laboral</td>\n",
       "      <td>Formacion Academica</td>\n",
       "      <td>Cursos complementarios</td>\n",
       "      <td>Aptitudes</td>\n",
       "      <td>Datos personales</td>\n",
       "      <td>Referencias laborales</td>\n",
       "      <td>Logros</td>\n",
       "      <td>Hobbies</td>\n",
       "      <td>Emprendimientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Resumen</td>\n",
       "      <td>Experiencia Profesional</td>\n",
       "      <td>Educacion</td>\n",
       "      <td>Cursos</td>\n",
       "      <td>Conocimientos</td>\n",
       "      <td>Contacto</td>\n",
       "      <td>Referencias</td>\n",
       "      <td>Logros y objetivos alcanzados</td>\n",
       "      <td>Hobbies, actividades y deportes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Biografía Profesional</td>\n",
       "      <td>Antecedentes Laborales</td>\n",
       "      <td>Formacion</td>\n",
       "      <td>Cursos y seminarios</td>\n",
       "      <td>Habilidades</td>\n",
       "      <td>Detalles personales</td>\n",
       "      <td>Referencias personales</td>\n",
       "      <td>Metas profesionales</td>\n",
       "      <td>Interes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sobre mi</td>\n",
       "      <td>Experiencia</td>\n",
       "      <td>Estudios</td>\n",
       "      <td>Talleres</td>\n",
       "      <td>Skills</td>\n",
       "      <td>Informacion de contacto</td>\n",
       "      <td>Mis referencias</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Areas de interes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acerca de mi</td>\n",
       "      <td>Experiencias</td>\n",
       "      <td>Formacion educativa</td>\n",
       "      <td>Seminarios</td>\n",
       "      <td>Cualificaciones</td>\n",
       "      <td>Otros antecedentes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Miscelaneo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Perfil              Experiencia            Educacion  \\\n",
       "0                 Perfil      Experiencia Laboral  Formacion Academica   \n",
       "1                Resumen  Experiencia Profesional            Educacion   \n",
       "2  Biografía Profesional   Antecedentes Laborales            Formacion   \n",
       "3               Sobre mi              Experiencia             Estudios   \n",
       "4           Acerca de mi             Experiencias  Formacion educativa   \n",
       "\n",
       "                   Cursos      Habilidades                 Contacto  \\\n",
       "0  Cursos complementarios        Aptitudes         Datos personales   \n",
       "1                  Cursos    Conocimientos                 Contacto   \n",
       "2     Cursos y seminarios      Habilidades      Detalles personales   \n",
       "3                Talleres           Skills  Informacion de contacto   \n",
       "4              Seminarios  Cualificaciones       Otros antecedentes   \n",
       "\n",
       "              Referencias                         Logros  \\\n",
       "0   Referencias laborales                         Logros   \n",
       "1             Referencias  Logros y objetivos alcanzados   \n",
       "2  Referencias personales            Metas profesionales   \n",
       "3         Mis referencias                            NaN   \n",
       "4                     NaN                            NaN   \n",
       "\n",
       "                           Hobbies  Emprendimientos  \n",
       "0                          Hobbies  Emprendimientos  \n",
       "1  Hobbies, actividades y deportes              NaN  \n",
       "2                          Interes              NaN  \n",
       "3                 Areas de interes              NaN  \n",
       "4                       Miscelaneo              NaN  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "path_secciones_dic = '/home/erwin/Genoma/cv-parser/parser/CSVs/Secciones CVs_buscador.csv'\n",
    "\n",
    "secciones_dic = pd.read_csv(path_secciones_dic)\n",
    "secciones_dic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiencia = secciones_limpio(secciones_dic.Experiencia)\n",
    "perfil = secciones_limpio(secciones_dic.Perfil)\n",
    "educacion = secciones_limpio(secciones_dic.Educacion)\n",
    "cursos = secciones_limpio(secciones_dic.Cursos)\n",
    "habilidades = secciones_limpio(secciones_dic.Habilidades) \n",
    "contacto = secciones_limpio(secciones_dic.Contacto)\n",
    "referencias = secciones_limpio(secciones_dic.Referencias)\n",
    "logros = secciones_limpio(secciones_dic.Logros)\n",
    "hobbies = secciones_limpio(secciones_dic.Hobbies)\n",
    "\n",
    "otros = perfil + educacion + cursos + habilidades + contacto + referencias + logros+ hobbies\n",
    "#otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_experiencia(cv_text):\n",
    "    linea_experiencia = False\n",
    "    siguiente_seccion = False\n",
    "    parrafo = ''\n",
    "    for line in cv_txt:\n",
    "        line_np = re.sub(r'[^\\w\\s]','', line)\n",
    "        l = sum([i.strip(string.punctuation).isalpha() for i in line_np.split()])\n",
    "        chunks = re.split(' +', line)\n",
    "        linea =''\n",
    "        for word in chunks:\n",
    "            linea += word.lower() + ' '\n",
    "\n",
    "        if ((len(line.strip()) == 0 or l > 4) and linea_experiencia == False):\n",
    "            continue\n",
    "\n",
    "\n",
    "        linea = \" \".join(linea.split())\n",
    "        for experiencia in experiencia_list:\n",
    "            linea_np = re.sub(r'[^\\w\\s]','', linea)\n",
    "            experiencia_np = re.sub(r'[^\\w\\s]','', experiencia)\n",
    "            linea_un = \"\".join(unidecode.unidecode(linea_np).split())\n",
    "            experiencia_un = \"\".join(unidecode.unidecode(experiencia_np).split())\n",
    "            if experiencia_un.lower() == linea_un.lower():\n",
    "                #print('He pillado la seccion')\n",
    "                linea_experiencia = True\n",
    "                #continue\n",
    "\n",
    "\n",
    "        for otro in otros:\n",
    "            otro_np = re.sub(r'[^\\w\\s]','', otro)\n",
    "            linea_np = re.sub(r'[^\\w\\s]','', linea)\n",
    "            otro_un = unidecode.unidecode(otro_np)\n",
    "            linea_un = unidecode.unidecode(linea_np)\n",
    "\n",
    "            if linea_un.lower() == otro_un.lower() and linea_experiencia:\n",
    "                #print(linea_un.upper())\n",
    "                siguiente_seccion = True\n",
    "                break\n",
    "\n",
    "        if siguiente_seccion:\n",
    "            break\n",
    "\n",
    "        if linea_experiencia == True and siguiente_seccion == False:\n",
    "            parrafo += linea + '\\n'\n",
    "\n",
    "    return parrafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiencia laboral\\nde 21/01/2019 a la fecha\\nminera antucoya, superintendencia de geotecnia\\ncargo ocupado practicante 21/01/2019 01/03/2019\\n\\nactualización continua de base de datos estructurales, mediante mapeos de bancos y\\ndatos i-site.\\n\\ncreación de script para software vulcan, a fin de realizar actualizaciones a modelos de\\ncorto plazo con unidades geotécnicas.\\n\\ncreación de plantas del modelo de corto plazo, con unidades geotécnicas y\\ndensidades, para área de perforación y tronadura.\\n\\nanálisis de potenciales mecanismos de inestabilidad en software tangram de bancos\\nextraídos y bancos por extraer.\\n\\ncreación de superficies compuestas diseño de fase avance de mina para análisis\\nde mecanismos de inestabilidad en bancos por extraer.\\ncargo ocupado memorista 01/03/2019 actualidad\\n\\nactualización de estructuras mayores e intermedias, a partir de mapeos de bancos y\\ndatos complementarios i-site obtenidos en escaneos de alta resolución\\n\\nmodelamiento de superficies de fallas actualizadas en software leapfrog geo.\\n\\nanálisis de potenciales mecanismos de inestabilidad en software tangram, para\\ndiseños de fases, bancos extraídos y bancos por extraer.\\n\\nelaboración de procedimiento para la identificación de mecanismos de inestabilidad.\\n\\nelaboración de cartilla para mapeo en detalle de estructuras mayores e intermedias en\\nbancos.\\n\\n5 días de mapeo estructural de bancos asistiendo a consultor john fedorowich\\n'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1586786838778-CV_Postulaciones'\n",
    "path = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1571410211245-CV_Mariana_Wong'\n",
    "path = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1571275460067-CV_Gonzalo_Vásquez'\n",
    "path = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1569272625085-CV_Arianne.R.'\n",
    "path = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1568522376573-CV_Jorge_Berna_Espinoza'\n",
    "path = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1569025536095-19.08.19_CV_Nicolas_Achondo'\n",
    "path = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1568672819667-CV_CAMILO_BUSTAMANTE_SANTANDERR'\n",
    "path = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1566924682475-CV_CatalinaZunigaBilbao'\n",
    "path = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1566746473048-Patricio_Mendez-CV'\n",
    "file = path\n",
    "cv_txt = open(file, \"r\")\n",
    "\n",
    "extraer_experiencia(cv_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1566924682475-CV_CatalinaZunigaBilbao'\n",
    "cv_txt = open(file, \"r\").read()\n",
    "#print(cv_txt)\n",
    "def extraer_perfil(cv_text):\n",
    "    otros = educacion + cursos + habilidades + contacto + referencias + logros+ hobbies + experiencia\n",
    "    n = -1\n",
    "    siguiente_seccion = False\n",
    "    parrafo = ''\n",
    "    #print(cv_text)\n",
    "    #text_1 = cv_text\n",
    "    n_linea = 0\n",
    "    for line in cv_text.splitlines():\n",
    "        n += 1\n",
    "        #print(line)\n",
    "        for resumen in perfil:\n",
    "            linea_np = re.sub(r'[^\\w\\s]','', line)\n",
    "            resumen_np = re.sub(r'[^\\w\\s]','', resumen)\n",
    "            linea_un = \"\".join(unidecode.unidecode(linea_np).split())\n",
    "            resumen_un = \"\".join(unidecode.unidecode(resumen_np).split())\n",
    "            if resumen_un.lower() == linea_un.lower():\n",
    "                linea_resumen = True\n",
    "                print('pille linea resumen')\n",
    "                print(resumen_un)\n",
    "                n_linea = n\n",
    "                break\n",
    "    print(n_linea)\n",
    "    #print(cv_txt)   \n",
    "    for line in cv_text.splitlines()[n_linea:-1]:   \n",
    "        #print('entre aca')\n",
    "        chunks = re.split(' +', line)\n",
    "        linea =''\n",
    "        for word in chunks:\n",
    "            linea += word.lower() + ' '\n",
    "\n",
    "        linea = \" \".join(linea.split())\n",
    "\n",
    "\n",
    "\n",
    "        for otro in otros:\n",
    "            otro_np = re.sub(r'[^\\w\\s]','', otro)\n",
    "            linea_np = re.sub(r'[^\\w\\s]','', linea)\n",
    "            otro_un = unidecode.unidecode(otro_np)\n",
    "            linea_un = unidecode.unidecode(linea_np)\n",
    "\n",
    "            if linea_un.lower() == otro_un.lower():\n",
    "                #print(linea_un.upper())\n",
    "                siguiente_seccion = True\n",
    "                break\n",
    "\n",
    "        if siguiente_seccion:\n",
    "            break\n",
    "\n",
    "        if  siguiente_seccion == False:\n",
    "            parrafo += linea + '\\n'\n",
    "\n",
    "    return parrafo\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "experiencia = secciones_limpio(secciones_dic.Experiencia)\n",
    "perfil = secciones_limpio(secciones_dic.Perfil)\n",
    "educacion = secciones_limpio(secciones_dic.Educacion)\n",
    "cursos = secciones_limpio(secciones_dic.Cursos)\n",
    "habilidades = secciones_limpio(secciones_dic.Habilidades) \n",
    "contacto = secciones_limpio(secciones_dic.Contacto)\n",
    "referencias = secciones_limpio(secciones_dic.Referencias)\n",
    "logros = secciones_limpio(secciones_dic.Logros)\n",
    "hobbies = secciones_limpio(secciones_dic.Hobbies)\n",
    "\n",
    "otros = perfil + educacion + cursos + habilidades + contacto +  logros+ hobbies + experiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1566924682475-CV_CatalinaZunigaBilbao'\n",
    "cv_txt = open(file, \"r\").read()\n",
    "\n",
    "def extraer_referencias(cv_text):\n",
    "    linea_referencia = False\n",
    "    siguiente_seccion = False\n",
    "    parrafo = ''\n",
    "    for line in cv_txt.splitlines():\n",
    "        line_np = re.sub(r'[^\\w\\s]','', line)\n",
    "        l = sum([i.strip(string.punctuation).isalpha() for i in line_np.split()])\n",
    "        chunks = re.split(' +', line)\n",
    "        linea =''\n",
    "        for word in chunks:\n",
    "            linea += word.lower() + ' '\n",
    "\n",
    "        if ((len(line.strip()) == 0 or l > 4) and linea_referencia == False):\n",
    "            continue\n",
    "\n",
    "        #print(linea + '\\n')\n",
    "        linea = \" \".join(linea.split())\n",
    "\n",
    "        for referencia in referencias:\n",
    "            linea_np = re.sub(r'[^\\w\\s]','', linea)\n",
    "            referencia_np = re.sub(r'[^\\w\\s]','', referencia)\n",
    "            linea_un = \"\".join(unidecode.unidecode(linea_np).split())\n",
    "            referencia_un = \"\".join(unidecode.unidecode(referencia_np).split())\n",
    "            if referencia_un.lower() == linea_un.lower():\n",
    "                print('He pillado la seccion')\n",
    "                linea_referencia = True\n",
    "                #print(linea.UPPER())\n",
    "                #continue\n",
    "\n",
    "\n",
    "        for otro in otros:\n",
    "            otro_np = re.sub(r'[^\\w\\s]','', otro)\n",
    "            linea_np = re.sub(r'[^\\w\\s]','', linea)\n",
    "            otro_un = unidecode.unidecode(otro_np)\n",
    "            linea_un = unidecode.unidecode(linea_np)\n",
    "\n",
    "            if linea_un.lower() == otro_un.lower() and linea_referencia:\n",
    "                print(linea_un)\n",
    "                siguiente_seccion = True\n",
    "                break\n",
    "\n",
    "        if siguiente_seccion:\n",
    "            break\n",
    "\n",
    "        if linea_referencia == True and siguiente_seccion == False:\n",
    "            parrafo += linea + '\\n'\n",
    "            \n",
    "    if len(parrafo.splitlines())>1:\n",
    "        parrafo = \"\\n \".join([str(x) for x in parrafo.splitlines()[1:-1]])\n",
    "        \n",
    "\n",
    "    return parrafo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He pillado la seccion\n",
      "maría cristina vallejos. líder zona centro sur productividad,\n",
      " vicepresidencia de proyectos, codelco- 975496775\n",
      " victor encina. jefe de carrera ingeniería civil de minas utfsm-\n",
      " 98875257\n",
      " milka casanegra. presidenta rim, ingeniera tronadura qa/qc\n"
     ]
    }
   ],
   "source": [
    "print(extraer_referencias(cv_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Experiencia profesional\n",
      "2018-2019\n",
      "Red de ingenieras de Minas de Chile-RIM\n",
      "Cargo\n",
      "Fundadora\n",
      "Función\n",
      "Planificación, coordinación y\n",
      "ejecución de actividades\n",
      "relacionadas con el empoderamiento femenino y la inclusión de\n",
      "género en la industria minera. Meet and Greet, Charla\n",
      "habilidades blandas, Reuniones con empresas generando\n",
      "alianzas\n",
      "2019\n",
      "Triconos Mineros\n",
      "Cargo\n",
      "Memorista\n",
      "Función\n",
      "Estudio y análisis de la información en el área de perforación de\n",
      "Minera Los Pelambres, con el objetivo de elaborar un plan de\n",
      "mejoramiento de la gestión de la calidad de la información.\n",
      "Feb. 2018\n",
      "Nueva Ancortecmin S.a\n",
      "Cargo Practica Industrial\n",
      "Función\n",
      "Seguimiento de costos y presupuestos con el objetivo de\n",
      "levantar aprendizajes y oportunidades de mejora.\n",
      "Enero 2014 Endesa S.a\n",
      "Cargo Practica Industrial\n",
      "Función\n",
      "Desarrollo de tareas relacionadas con el control automático e\n",
      "instrumentación de plantas termoeléctricas, revisando planos y\n",
      "correcta ejecución de funcionamiento de sistemas.\n",
      "Catalina\n",
      "Zúñiga\n",
      "Bilbao\n",
      "Avenida Troncal\n",
      "San Francisco\n",
      "2140-9\n",
      "Puente Alto\n",
      "Santiago\n",
      "catalinazunigabilba\n",
      "o@gmail.com\n",
      "56978832072\n",
      "Licencia de\n",
      "Conducir Clase B\n",
      "Formación académica\n",
      "2015-Actualidad\n",
      "Universidad Técnica Federico Santa María\n",
      "Estudiante Ingeniería Civil de Minas\n",
      "2018-Actualidad\n",
      "Instituto Chileno Británico de Cultura\n",
      "Intermediate- Level 8.\n",
      "2010-2014\n",
      "Universidad Técnica Federico Santa María\n",
      "Estudiante Ingeniería Civil Eléctrica\n",
      "2006-2009\n",
      "Liceo Carmela Carvajal de Prat\n",
      "Enseñanza Media\n",
      "Breve descripción de la formación adquirida\n",
      "Profesión / Área profesional\n",
      "Memorista de Ingeniería Civil de Minas, con altas capacidades de\n",
      "aprendizaje, liderazgo, trabajo en equipo y de resolución de problemas.\n",
      "Proactiva, responsable y comprometida con la mejora continua, busca\n",
      "formar parte de equipos de trabajo en que pueda fortalecer sus\n",
      "conocimientos y experiencias técnicas, junto con aportar un trabajo\n",
      "dinámico en pos de lograr metas establecidas. Disponibilidad para trabajar\n",
      "en regiones y en sistema de turnos.\n",
      "Idiomas\n",
      "Inglés Medio-Alto. Nivel 8 en Instituto Chileno Británico de Cultura.\n",
      "Herramientas Informáticas\n",
      "ArcMap. Nivel alto\n",
      "Isatis. Nivel medio\n",
      "Rocky. Nivel alto\n",
      "Minesight. Nivel medio\n",
      "Dips. Nivel medio\n",
      "Ventsim. Nivel medio\n",
      "Cad. Nivel alto\n",
      "MapInfo. Nivel básico\n",
      "Sap B1. Nivel medio\n",
      "Actividades Extracurriculares\n",
      "Ayudante activa en la universidad desde mediados del año 2012, donde ha\n",
      "desarrollado ayudantías y apoyo al desarrollo de las asignaturas\n",
      "\n",
      "Geología Económica  2do semestre 2018\n",
      "\n",
      "Geología estructural y geotecnia  2017-2018\n",
      "\n",
      "Procesamiento de minerales 2  2do semestre 2017\n",
      "\n",
      "Física General Ii  2do semestre 2015-2017\n",
      "\n",
      "Geología General 2do semestre 2016\n",
      "\n",
      "Física General Iv 1er Semestre de 2015\n",
      "\n",
      "Física General Iii 1er Semestre de 2013\n",
      "\n",
      "Actividades Extraprogramáticas\n",
      "Deportes\n",
      " Premiada como Mejor Deportista Utfsm, San Joaquín, 2010.\n",
      " Cardiobox, Gimnasio Energy.\n",
      " Grupo Hiking Santiago 2018-2019.\n",
      " Taller Folclore, Universidad Federico Santa María 2014-2017.\n",
      "\n",
      "Voluntariados\n",
      " Guía de Catequesis para niños, Iglesia Santa Teresa de Calcuta, 2014-\n",
      "2015.\n",
      " Apoyo Pastoral Social, Iglesia Santa Teresa de Calcuta, 2018-2019\n",
      "Hobbies\n",
      "Trekking, bordado, viajes.\n",
      ".\n",
      "Cursos y Seminarios\n",
      " Curso Inducción Minera Los Pelambres, 2019.\n",
      " Curso Manejo defensivo, Minera Los Pelambres, 2019.\n",
      " CursoODI-DAS-2019 Minera Los Pelambres, 2019\n",
      " Curso Caída de Rocas, Minera Los Pelambres, 2019\n",
      " Capacitación Rocky Dem, 2017\n",
      " Taller MapInfo en la Exploración minera 2017\n",
      "\n",
      "Referencias\n",
      " María Cristina Vallejos. Líder zona centro sur productividad,\n",
      "Vicepresidencia de Proyectos, Codelco- 975496775\n",
      " Victor Encina. Jefe de Carrera Ingeniería Civil de Minas Utfsm-\n",
      "98875257\n",
      " Milka Casanegra. Presidenta Rim, Ingeniera Tronadura Qa/qc\n",
      "Minera Los Pelambres. - 75173563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cv_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python [conda env:cv_parser] *",
   "language": "python",
   "name": "conda-env-cv_parser-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
