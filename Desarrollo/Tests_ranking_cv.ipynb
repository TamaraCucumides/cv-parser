{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import es_core_news_sm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import es_core_news_sm\n",
    "import itertools\n",
    "from nltk.stem import SnowballStemmer\n",
    "import textacy\n",
    "import regex\n",
    "import json\n",
    "import unidecode\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import yaml\n",
    "wordvectors_file_vec ='/home/erwin/Genoma/cv-parser/parser/embeddings/fasttext-sbwc.3.6.e20.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantidad = 300000\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_json = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_seccionado'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "#print(json_files)  # for me this prints ['foo.json']\n",
    "jsons = []\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js)) as json_file:\n",
    "        jsons.append(json.load(json_file))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marketing gestión comercial persona proactiva ordenar alto sentir responsabilidad equipar fundamental empresa orientar resultar tangible laboral adecco peru cargo gerente desarrollo fecha 01/04/2019 fecho responsable manejar cartero top cliente responsable cumplimiento objetivo venta líneo negociar garantizar rentabilidad nivel servicio desarrollo convenio diverso institución implementación canal post-venta cliente extranjero enfoque negociar cliente extranjero desarrollar servicio medir cliente adecco peru cargo gerente sucursal fecha 01/01/2017 31/03/2019 responsable gestión integral sucursal responsable cumplimiento objetivo venta líneo negociar garantizar rentabilidad nivel servicio desarrollo ejecución plan negocio venta kpis estrategia comercial orientar captación mantenimiento retención cliente cargo ejecutivo comercial fecha 25/03/2015 31/12/2016 funciones ejecutar estrategia comercial orientar captación mantenimiento retención cliente sucursal identificar información negocio relevante oportunidad comercial seguimiento propuesta comercial presentar cliente mantener comunicación estable periódico cliente realizar visitar mantenimiento seguimiento gestión facturación cobranza cliente elaborar propuesta comercial acorde servicio logros premio latam gestión comercial 2015 freno s.a cargo coordinador inteligencia comercial fecha 01/08/2014 20/03/2015 supervisión fuerza ventas lima estudio mercar lanzamiento producto mejorar portafolio producto servicio post-venta distribuidor minorista prospección punto venta lima metropolitana bolsa valores lima cargo ejecutivo comercial fecha 01/12/2012 31/07/2014 ejecutar estrategia comercial orientar captación mantenimiento retención cliente identificar información negocio relevante oportunidad comercial elaborar propuesta comercial acorde servicio logros meta anual 2013 superar 30 idiomas ingles nivel avanzado '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsons[0]['experiencia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_process' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b903ad61f894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'experiencia'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pre_process' is not defined"
     ]
    }
   ],
   "source": [
    "sent = pre_process(jsons[0]['experiencia'])\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lematizar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0681ae80cd58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msent_lem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlematizar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_lem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lematizar' is not defined"
     ]
    }
   ],
   "source": [
    "sent_lem = lematizar(sent)\n",
    "print(sent_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lematizar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c1fae7038a59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlematizar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ingles'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lematizar' is not defined"
     ]
    }
   ],
   "source": [
    "lematizar('ingles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    '''Generate Vectora for sentences.'''\n",
    "    M = []\n",
    "    for w in s.split():\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v/np.sqrt((v**2).sum())\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    '''Return Cosine Similarity.'''\n",
    "    return  np.dot(vec1,vec2)/(np.linalg.norm(vec1)* np.linalg.norm(vec2))\n",
    "\n",
    "def get_closest(word, n):\n",
    "    '''Get n most similar words by words.'''\n",
    "    #This function can easily be expanded to get similar words to phrases--\n",
    "    #using sent2vec() method defined in WithWord2Vec notebook. \n",
    "    word = word.lower()\n",
    "    words = [word]\n",
    "    similar_vals = [1]\n",
    "    try:\n",
    "        similar_list = model.most_similar(positive=[word],topn=n)\n",
    "        \n",
    "        for tupl in similar_list:\n",
    "            words.append(tupl[0])\n",
    "            similar_vals.append(tupl[1])\n",
    "    except:\n",
    "        #If word not in vocabulary return same word and 1 similarity-- \n",
    "        #see initialisation of words, similarities.\n",
    "        pass\n",
    "    \n",
    "    return words, similar_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bd0ad79f7ac>:11: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return v/np.sqrt((v**2).sum())\n"
     ]
    }
   ],
   "source": [
    "frase_1 = 'software developer'\n",
    "frase_2 = 'web developer'\n",
    "\n",
    "\n",
    "vector_sentence_1 = sent2vec(frase_1)\n",
    "vector_sentence_2 = sent2vec(frase_2)\n",
    "similitud = cosine_sim(vector_sentence_1,vector_sentence_2)\n",
    "\n",
    "print(similitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ingle'], [1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest(word= 'ingle', n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e99def79f303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tradición'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tradicional'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.similarity('tradición', 'tradicional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'educar'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import es_core_news_md\n",
    "nlp = es_core_news_md.load()\n",
    "nlp_text = nlp('educado')\n",
    "nlp_text[0].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_description = '''ingeniería máster postgrado excel desarrollo gestión comercial experiencia manejo clientes\n",
    "emprendimiento liderar equipos planificar organizar dirigir trabajo presión seguimiento KPI inglés '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/prateekguptaiiitk/Resume_Filtering/blob/develop/Scoring/CV_ranking.ipynb\n",
    "word_value = {}\n",
    "similar_words_needed = 1\n",
    "for word in prc_description.split():\n",
    "    similar_words, similarity = get_closest(word, similar_words_needed)\n",
    "    for i in range(len(similar_words)):\n",
    "        word_value[similar_words[i]] = word_value.get(similar_words[i], 0)+similarity[i]\n",
    "        #print(similar_words[i], word_value[similar_words[i]])\n",
    "        #print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ingeniería', 'máster', 'postgrado', 'excel', 'desarrollo', 'gestión', 'comercial', 'experiencia', 'manejo', 'clientes', 'emprendimiento', 'liderar', 'equipos', 'planificar', 'organizar', 'dirigir', 'trabajo', 'presión', 'seguimiento', 'kpi', 'inglés'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_value.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### ahora veamos si resulta el ranking\n",
    "#frecuencia de término – frecuencia inversa de documento \n",
    "#Tf-idf\n",
    "#Para calcular este ranking es mejor tener las secciones skills y experiencia\n",
    "#con el fin de calcular esta metrica usando la ocurrencia de las palabras\n",
    "# Tenemos todos los CV's y una descripción del cargo, a este descripcion del cargo\n",
    "# tiene N palabras, le buscamos 2 palabras parecidas, generando una descripcion\n",
    "# de N*2\n",
    "\n",
    "# Usando esta nuevo set de palabras de descripcion, recorremos todos los cvs contando \n",
    "# la ocurrencia de estas palabras en cada documento, y luego se genera un ranking usando Tf-idf\n",
    "# La pregunta es: ¿Lo haré sobre el documento entero? o ¿Trataré de seccionar y ocupar ciertas secciones?\n",
    "\n",
    "no_of_cv = len(jsons)\n",
    "#print(no_of_cv)\n",
    "\n",
    "count = {}\n",
    "idf = {}\n",
    "for word in word_value.keys():\n",
    "    count[word] = 0\n",
    "    for i in range(no_of_cv):\n",
    "        try:\n",
    "            if word in jsons[i]['skills'] or word in jsons[i]['experiencia'] or word in jsons[i]['educación']:\n",
    "                #print('entre')\n",
    "                count[word] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    idf[word] = math.log((no_of_cv+1)/(1+count[word]))\n",
    "    #print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculemos usando similitud\n",
    "\n",
    "\n",
    "\n",
    "def calculo_similitud(word1, array_palabras):\n",
    "    n_veces = 0\n",
    "    for word in array_palabras:\n",
    "        try:\n",
    "            sim = model.similarity(word, word1)\n",
    "            #print(word)\n",
    "            #print(sim)\n",
    "            if sim > 0.5:\n",
    "                n_veces += 1\n",
    "            else:\n",
    "                continue\n",
    "        except: #No estaba la palabra\n",
    "            pass\n",
    "        \n",
    "    return n_veces\n",
    "\n",
    "\n",
    "\n",
    "word1 = 'feliz'\n",
    "array = ['alegre', 'contento', 'taladro', 'juguete']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n = calculo_similitud(word1, array)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsons[0]['skills'].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_process' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-203c73b513da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_of_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# eliminación de stopwords y quizas lematizacion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mskill_pro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'skills'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mexpe_pro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'experiencia'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0medu_pro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'educación'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pre_process' is not defined"
     ]
    }
   ],
   "source": [
    "no_of_cv = len(jsons)\n",
    "\n",
    "count = {}\n",
    "idf = {}\n",
    "for word in word_value.keys():\n",
    "    count[word] = 0\n",
    "    for i in range(no_of_cv):\n",
    "        # eliminación de stopwords y quizas lematizacion\n",
    "        skill_pro = pre_process(jsons[i]['skills']) \n",
    "        expe_pro = pre_process(jsons[i]['experiencia'])\n",
    "        edu_pro = pre_process(jsons[i]['educación'])\n",
    "        \n",
    "        if calculo_similitud(word, skill_pro.split()) or calculo_similitud(word, expe_pro.split()) or calculo_similitud(word, edu_pro.split()):\n",
    "            count[word] += 1\n",
    "\n",
    "\n",
    "    idf[word] = math.log((no_of_cv+1)/(1+count[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score = {}\n",
    "for i in range(no_of_cv):\n",
    "    score[i] = 0\n",
    "    skill_pro = pre_process(jsons[i]['skills']) \n",
    "    expe_pro = pre_process(jsons[i]['experiencia'])\n",
    "    edu_pro = pre_process(jsons[i]['educación'])\n",
    "    for word in word_value.keys():\n",
    "        \n",
    "        \n",
    "        \n",
    "        tf = 1 + calculo_similitud(word, skill_pro.split()) + calculo_similitud(word, expe_pro.split()) + calculo_similitud(word, edu_pro.split())\n",
    "\n",
    "        score[i] += word_value[word]*tf*idf[word]\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = []\n",
    "for i in range(no_of_cv):\n",
    "    sorted_list.append((score[i], jsons[i]['nombre archivo']))\n",
    "    \n",
    "sorted_list.sort(reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "import spacy\n",
    "import es_core_news_sm\n",
    "\n",
    "\n",
    "def pre_process(corpus):\n",
    "    corpus = corpus.lower()\n",
    "\n",
    "    stopset = stopwords.words('spanish') + list(string.punctuation)\n",
    "\n",
    "    corpus = \" \".join([i for i in word_tokenize(corpus) if i not in stopset])\n",
    "    # remove non-ascii characters\n",
    "    corpus = unidecode(corpus)\n",
    "    return corpus\n",
    "\n",
    "def lematizar(frase):\n",
    "    nlp = es_core_news_sm.load()\n",
    "    doc = nlp(frase)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    return lemmas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pre_process('educación básica')\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lematizar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9c19ed21f1fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlematizar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ingeniero'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lematizar' is not defined"
     ]
    }
   ],
   "source": [
    "test = lematizar('ingeniero')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lematizar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c1fa7b6b1de4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlematizar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ingeniero'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlematizar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ingenieria'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lematizar' is not defined"
     ]
    }
   ],
   "source": [
    "lematizar('ingeniero') in lematizar('ingenieria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fab761ec8ad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ingeniero'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ingenieria'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.similarity('ingeniero', 'ingenieria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7ab751914bc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#stop_words = set(stopwords.words('spanish'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#stemmed_clave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "   # stemmed_claves = [stemmer.stem(token) for token in palabras_claves]\n",
    "#stop_words = set(stopwords.words('spanish')) \n",
    "\n",
    "[stemmer.stem(x) for x in test] \n",
    "#stemmed_clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('ingeniería') in stemmer.stem('ingeniería')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def cargar_dict(path):\n",
    "    with open(path) as f:  \n",
    "        array = [x.strip() for x in f]\n",
    "        c = [x for x in array if x != ''] # '' aparece cuando hay lines vacias\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/erwin/Genoma/cv-parser/parser/diccionarios/stop_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1ebe5f0ad412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnewStopWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcargar_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/erwin/Genoma/cv-parser/parser/diccionarios/stop_words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spanish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewStopWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-3073338ffaf6>\u001b[0m in \u001b[0;36mcargar_dict\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcargar_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# '' aparece cuando hay lines vacias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/erwin/Genoma/cv-parser/parser/diccionarios/stop_words'"
     ]
    }
   ],
   "source": [
    "newStopWords = cargar_dict('/home/erwin/Genoma/cv-parser/parser/diccionarios/stop_words')\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "stopwords.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def pre_process(corpus, stopWords, enminiscula = True):\n",
    "    if enminiscula:\n",
    "        corpus = corpus.lower()\n",
    "    stopset = stopwords+ list(string.punctuation)\n",
    "\n",
    "    corpus = \" \".join([i for i in word_tokenize(corpus) if i not in stopset])\n",
    "    # remove non-ascii characters\n",
    "    #corpus = unidecode.unidecode(corpus)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'LazyCorpusLoader' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b06ed6a357df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UNIVERSIDAD Universidad universidad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-828814c2a94a>\u001b[0m in \u001b[0;36mpre_process\u001b[0;34m(corpus, stopWords, enminiscula)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0menminiscula\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstopset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'LazyCorpusLoader' and 'list'"
     ]
    }
   ],
   "source": [
    "pre_process('UNIVERSIDAD Universidad universidad', stopwords, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import es_core_news_sm\n",
    "def lematizar(frase):\n",
    "    '''\n",
    "    Esta función recibe un string y le aplica lematización:\n",
    "    ingeniero ---> ingenier\n",
    "    ingeniera ----> ingenier\n",
    "    '''\n",
    "    nlp = es_core_news_sm.load()\n",
    "    doc = nlp(frase)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' nos encontrar en lo búsqueda de socios comercial , parir administrar por completar importante punto de venta ( corner ) de retail de calzar .  has escuchar hablar de los franquicia y consignación ? pues este empresa trabajar a través de lo consignación , este querer decir que lo empresa te hacer entregar del punto de venta , correr con todo lo gasto de infraestructura , arrendar y todo lo mercadería y renovación de stock constante , apoyar de visual y marketing , etcétera .  buscamos vendedor que se encargar de lo administración por completar del punto de venta ( rrhh , seguimiento y gestión inventario , bodega , venta , etcétera ) . ofrecemos atractivo comisionar  los monto líquido promedio mensual a lo que poder acceder , posterior a lo gasto de administración ir desde : $ 600.000 a $ 1.000.000  requisitos :  -experiencia en venta ( dependiente o independiente )  -disponibilidad parir horario de mall  -motivación y orientación al lograr y a lo venta  -ambición y competitividad  -manejo concepto básico de contabilidad y legislación laboral  -excelente capacidad de administrar uno equipar .  -posee uno sociedad limitar ( si no contar con este , no ser impedimento , poner que ser tramitar simple que se realizar de manera on line )  buscamos personar que desear emprender , tener orientación a lo venta y el liderazgo , parir todo ellos este ser lo oportunidad parir iniciar uno negociar propio con el respaldar de uno marcar líder en su rubro .  '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "des = '''\n",
    "Nos encontramos en la búsqueda de Socios comerciales, para administrar por completo importantes puntos de ventas (corner) de retail de calzado.\n",
    "\n",
    "Has escuchado hablar de las franquicias y consignaciones? pues esta empresa trabaja a través de la consignación, esto quiere decir que la empresa te hace entrega del punto de venta, corriendo con todos los gastos de infraestructura, arriendo y toda la mercadería y renovación de stock constante, apoyo de visual y marketing,etc.\n",
    "Buscamos vendedores que se encarguen de la administración por completo del punto de venta (RRHH,seguimiento y gestión inventarios, bodegas, ventas, etc.). Ofrecemos atractivas comisiones\n",
    "\n",
    "Los montos líquidos PROMEDIO mensual a los que puedes acceder, posterior a los gastos de administración van desde: $600.000 a $1.000.000\n",
    "\n",
    "Requisitos:\n",
    "-Experiencia en ventas (dependiente o independiente)\n",
    "-Disponibilidad para horarios de mall\n",
    "-Motivación y orientación al logro y a la venta\n",
    "-Ambición y competitividad\n",
    "-Manejo conceptos básicos de contabilidad y legislación laboral\n",
    "-Excelente capacidad de administrar un equipo.\n",
    "-Posee una sociedad limitada (si no cuenta con esto, no es impedimento, puesto que es tramite simple que se realiza de manera on line)\n",
    "\n",
    "\n",
    "Buscamos personas que deseen emprender, tengan orientación a la venta y el liderazgo, para todos ellos esta es la oportunidad para iniciar un negocio propio con el respaldo de una marca líder en su rubro.\n",
    "'''\n",
    "\n",
    "\n",
    "f =lematizar(des)\n",
    "lematizado = ''\n",
    "for word in f:\n",
    "    lematizado += word +' '\n",
    "lematizado.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "def pre_process(corpus,  enminiscula= True):\n",
    "    '''\n",
    "    Entrada: texto, stopwords, enminiscula (opcional)\n",
    "    Salida:  texto\n",
    "    Funcion que se encarga de limpiar las stopwords de un texto\n",
    "    el parámetro opciona enminuscula si es verdadero,\n",
    "    transforma todo el texto a miniscula y elimina stopwords que esten en minuscula.\n",
    "    Cuando se usa false, el texto retornado mantendra capitalizacion original y \n",
    "    además se eliminan stop words especificas tales como: Pontificia, Universidad, Vitae, VITAE\n",
    "    Notar que stop_words.txt tiene stopwords en minisculas y capitalizada.\n",
    "    Esta propiedad de mantener la capitalización es útil en la detección de nombres.\n",
    "    '''\n",
    "    newStopWords = cargar_dict('/home/erwin/Genoma/cv-parser/parser/diccionarios/stop_words')\n",
    "    stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "    print(newStopWords)\n",
    "    stop = stopwords.extend(newStopWords)\n",
    "    \n",
    "\n",
    "    if enminiscula:\n",
    "        corpus = corpus.lower()\n",
    "    stopset = stopwords+ list(string.punctuation)\n",
    "\n",
    "    corpus = \" \".join([i for i in word_tokenize(corpus) if i not in stopset])\n",
    "    # remove non-ascii characters\n",
    "    #corpus = unidecode.unidecode(corpus)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_dict(path):\n",
    "    '''\n",
    "    Utilidad para cargar los diccionarios\n",
    "    '''\n",
    "    with open(path) as f:  \n",
    "        array = [x.strip() for x in f]\n",
    "        c = [x for x in array if x != ''] # '' aparece cuando hay lines vacias\n",
    "    return c\n",
    "\n",
    "file = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1586903416737-NewHTJ'\n",
    "cv_txt = cargar_dict(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \" \".join(cv_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/erwin/Genoma/cv-parser/parser/diccionarios/stop_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-617366e1a294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-92a080c851f5>\u001b[0m in \u001b[0;36mpre_process\u001b[0;34m(corpus, enminiscula)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mEsta\u001b[0m \u001b[0mpropiedad\u001b[0m \u001b[0mde\u001b[0m \u001b[0mmantener\u001b[0m \u001b[0mla\u001b[0m \u001b[0mcapitalización\u001b[0m \u001b[0mes\u001b[0m \u001b[0mútil\u001b[0m \u001b[0men\u001b[0m \u001b[0mla\u001b[0m \u001b[0mdetección\u001b[0m \u001b[0mde\u001b[0m \u001b[0mnombres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     '''\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mnewStopWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcargar_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/erwin/Genoma/cv-parser/parser/diccionarios/stop_words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spanish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewStopWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-16c0f261ac3f>\u001b[0m in \u001b[0;36mcargar_dict\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mUtilidad\u001b[0m \u001b[0mpara\u001b[0m \u001b[0mcargar\u001b[0m \u001b[0mlos\u001b[0m \u001b[0mdiccionarios\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     '''\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# '' aparece cuando hay lines vacias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/erwin/Genoma/cv-parser/parser/diccionarios/stop_words'"
     ]
    }
   ],
   "source": [
    "pre_process(t, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python [conda env:cv_parser] *",
   "language": "python",
   "name": "conda-env-cv_parser-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
