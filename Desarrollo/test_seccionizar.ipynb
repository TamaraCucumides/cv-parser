{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import es_core_news_sm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import es_core_news_md\n",
    "import itertools\n",
    "from nltk.stem import SnowballStemmer\n",
    "import textacy\n",
    "import regex\n",
    "import unidecode\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import re\n",
    "import json\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "re_c = re.compile(r'\\w+')\n",
    "wordvectors_file_vec ='/home/erwin/Genoma/cv-parser/parser/embeddings/fasttext-sbwc.3.6.e20.vec'\n",
    "nlp = es_core_news_md.load()\n",
    "cantidad = 500000\n",
    "\n",
    "#model = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file= '/home/erwin/Genoma/cv-parser/parser/resumes_pdf/prueba_genoma_experiencia.pdf'\n",
    "file ='/home/erwin/Genoma/cv-parser/parser/resumes_pdf/1574824594718-Ing._Comercial_Paula_Fetis_CV.pdf'\n",
    "#file ='/home/erwin/Genoma/cv-parser/resumes/1580248761709-CV_Sergio_Soto_Valdes.pdf'\n",
    "#file = '/home/erwin/Genoma/cv-parser/parser/resumes_pdf/1582718980347-CV_-_Mario_Espinoza1.pdf'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paula Alejandra Fetis Carrasco\n",
      "Rut 18.793.163-0\n",
      "Fecha de nacimiento 05/09/1994\n",
      "Dirección Warren Smith 70\n",
      "Ciudad, País Santiago, Chile\n",
      "Teléfono 56 9 98165994\n",
      "e-mail p.fetis@gmail.com\n",
      "Resumen Del Perfil Profesional\n",
      "Ingeniero Comercial de la Universidad del Desarrollo, con buen manejo de inglés e interés\n",
      "en área de innovación, finanzas y marketing. Me destaco por mi alta orientación al trabajo\n",
      "en equipo, liderazgo, obtención de resultados y motivación.\n",
      "Experiencia Laboral\n",
      "Mayo 2019 a\n",
      "Agosto 2019\n",
      "Rovin Santiago, Chile\n",
      "Área administrativa / Apoyo back office\n",
      " Analizar comportamiento de gastos mensuales\n",
      " Control de stock e instalaciones mensuales\n",
      "Julio 2018 a Abril\n",
      "2019\n",
      "Banco Itaú Corpbanca Santiago, Chile\n",
      "Área comercial\n",
      " Parte del programa de jóvenes profesionales, el cual consta de\n",
      "entrenamiento para área como ejecutiva de cuentas de la\n",
      "banca preferencial.\n",
      " Asesoría y evaluación de distintos productos y servicios\n",
      "bancarios de clientes.\n",
      " Manejo de cartera con aproximadamente 290 clientes.\n",
      "Febrero a Abril\n",
      "2018\n",
      "Santa Tienda Concepción, Chile\n",
      "Ventas\n",
      " Analizar el comportamiento de compra de los clientes\n",
      "mensualmente con el fin de controlar stock y venta de\n",
      "productos.\n",
      " Evaluar horarios de atención de acuerdo al flujo de clientes y\n",
      "ventas para disminuir costos de personal y aumentar clientes.\n",
      "Diciembre 2015,\n",
      "Diciembre 2016 y\n",
      "Diciembre 2017\n",
      "Universidad del Desarrollo Concepción, Chile\n",
      "Proceso Admisión / Atención al Cliente\n",
      " Parte del equipo encargado de orientar a los posibles nuevos\n",
      "alumnos de la carrera Ingeniería Comercial.\n",
      " Dar información acerca de la universidad, becas, carreras, vías\n",
      "de ingreso, entre otros.\n",
      "Octubre 2014 a\n",
      "Octubre 2017\n",
      "Ingeniería Comercial Udd. Concepción, Chile\n",
      "Team Admisión\n",
      " Asistir a actividades de difusión de la carrera en representación\n",
      "de la coordinadora de carrera.\n",
      " Contarle a alumnos de establecimientos educacionales mi\n",
      "experiencia en la carrera a través de los años.\n",
      "Agosto 2017 a\n",
      "Noviembre 2017\n",
      "Confecciones Creaciones Jacqueline Concepción, Chile\n",
      "Práctica Profesional / Marketing\n",
      " Realizar estudio de la competencia sobre el uso de las redes\n",
      "sociales y plataformas web.\n",
      " Gestionar redes sociales.\n",
      "Marzo 2017 a\n",
      "Julio 2017\n",
      "Huachipato Fc Concepción, Chile\n",
      "Práctica Profesional / Gestión\n",
      " Encargada administrativa de escuelas filiales del club.\n",
      " Ordenar y actualizar información de cada escuela a nivel\n",
      "nacional.\n",
      "Agosto 2016 a\n",
      "Diciembre 2016\n",
      "Universidad del Desarrollo Concepción, Chile\n",
      "Ayudante Gestión de personas / Recursos Humanos\n",
      " Formular, evaluar y corregir pruebas efectuadas tanto en clases\n",
      "como en las ayudantías.\n",
      " Guiar a los alumnos en el desarrollo de su trabajo final para el\n",
      "semestre.\n",
      "Formación Académica\n",
      "2013 - 2017\n",
      "Universidad del Desarrollo. Concepción, Chile\n",
      "Carrera Ingeniería Comercial\n",
      "Titulada\n",
      "2015\n",
      "Universidad del Desarrollo. Concepción, Chile\n",
      "Minor en Innovación.\n",
      "Competencias Técnicas\n",
      "Software\n",
      "- Microsoft Excel. Nivel Avanzado.\n",
      "- Microsoft Power Point. Nivel Avanzado.\n",
      "- Microsoft Word. Nivel Avanzado.\n",
      "Idiomas\n",
      "- Inglés escrito, oral y lectura. Nivel Intermedio.\n",
      "- Italiano escrito, oral y lectura. Nivel Básico.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_text(path):\n",
    "    '''\n",
    "    Input: ruta hacia los archivos\n",
    "    Salida: Texto plano como string\n",
    "    Primero se extrae el texto usando fitz (PyMUPDF)\n",
    "    Luego, debido a que la salida no es perfecta, se limpia\n",
    "    eliminando todos los simbolos innecesarios.\n",
    "    Además para efectos de deteccion de nombres, \n",
    "    palabras completamente en mayusculas se capitalizan\n",
    "    sólo al principio. PAULA ---> Paula.\n",
    "    Esto hace más robusto la detección de entidades.\n",
    "    '''\n",
    "    with fitz.open(path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc: # text contiene todo el texto extraido desde el PDF\n",
    "            text += page.getText()\n",
    "        \n",
    "        text_2 = '' #texto sin mayusculas y saltos innecesarias \n",
    "        for line in text.splitlines():\n",
    "            if not line.strip(): #si la linea esta vacia, saltar\n",
    "                continue\n",
    "            line_2=''\n",
    "            for word in line.split():\n",
    "                if word.isupper(): # Si la palabra esta completamente en mayuscula\n",
    "                    line_2 += word.capitalize()+' '\n",
    "\n",
    "                else:\n",
    "                    line_2 += word+ ' '\n",
    "            text_2 += \" \".join(line_2.split()) +'\\n'\n",
    "        \n",
    "        simbolos = ' -,\\n./@' #Simbolos que se permiten, sirven para correo, links, etc.\n",
    "        text_clean = ' '\n",
    "        for char in text_2:\n",
    "            if (char.isalnum())| (char in simbolos): #Si el char es alphanumerico o es un simbolo permitido\n",
    "                text_clean += char\n",
    "       \n",
    "        text = text_clean\n",
    "    return text\n",
    "\n",
    "text = extract_text(file)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "País Santiago\n"
     ]
    }
   ],
   "source": [
    "nlp = es_core_news_sm.load()\n",
    "nlp_t = nlp(text)\n",
    "\n",
    "noun_chunks = list(nlp_t.noun_chunks)\n",
    "print(noun_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan los nombres de las secciones, se clasifican en \n",
    "# Perfil, Experiencia, Formación académica y skills\n",
    "\n",
    "seccion_csv = '/home/erwin/Genoma/cv-parser/parser/CSVs/seccionesCV.csv'\n",
    "secciones = pd.read_csv(seccion_csv, header = 0)\n",
    "secciones.columns = secciones.loc[0] \n",
    "#secciones.columns\n",
    "\n",
    "\n",
    "secciones_dict = {\n",
    "    'extras' : [str(x.lower()) for x in secciones.Perfil.values if str(x)!= 'nan'],\n",
    "    'experiencia' : [str(x.lower()) for x in secciones['Experiencia '].values if str(x)!= 'nan'],\n",
    "    'educación' : [str(x.lower()) for x in secciones['Formación Académica'].values if str(x)!= 'nan'],\n",
    "    'skills' : [str(x.lower()) for x in secciones['skills'].values if str(x)!= 'nan']                   \n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch for debug\n",
    "flag_print = False\n",
    "\n",
    "# switch to clear existing data\n",
    "flag_clear = True\n",
    "\n",
    "#threshold value for determining section\n",
    "threshold = 0.45\n",
    "\n",
    "similar_to = secciones_dict\n",
    "\n",
    "\n",
    "list_of_sections = similar_to.keys()\n",
    "\n",
    "# Usando secciones_dict que tiene las secciones a buscar y palabras que describen esas secciones\n",
    "# se llevan aquellas palabras a su lema\n",
    "for section in list_of_sections:\n",
    "    new_list = []\n",
    "    \n",
    "    for word in similar_to[section]:\n",
    "        docx = nlp(word)\n",
    "        new_list.append(docx[0].lemma_)\n",
    "    \n",
    "        \n",
    "    similar_to[section] = list(set(new_list)) # se retorna lista de elementos unicos\n",
    "#pp.pprint(similar_to)\n",
    "\n",
    "\n",
    "# function to remove unnecessary symbols and stopwords \n",
    "def modify(word):\n",
    "    try:\n",
    "        symbols = '''~'`!@#$%^&*)(_+-=}{][|\\:;\",./<>?'''\n",
    "        mod_word = ''\n",
    "        \n",
    "        for char in word:\n",
    "            if (char not in symbols):\n",
    "                mod_word += char.lower()\n",
    "\n",
    "        docx = nlp(mod_word)\n",
    "\n",
    "        if (len(mod_word) == 0 or docx[0].is_stop):\n",
    "            return None\n",
    "        else:\n",
    "            return docx[0].lemma_\n",
    "    except:\n",
    "        return None # to handle the odd case of characters like 'x02', etc.\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def is_empty(line):\n",
    "    '''\n",
    "    Retorna un booleano correspondiendo a \n",
    "    si una linea esta vacia en términos de letras-números\n",
    "    '''\n",
    "    for c in line:\n",
    "        if (c.isalpha()):\n",
    "            return False\n",
    "    return True\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1581878742461-Curriculum_Joselyn_Mari_2020_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b4c0aa60c572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#file = 'resumes_text_output/prueba_genoma_experiencia'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1581878742461-Curriculum_Joselyn_Mari_2020_1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcv_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprevious_section\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m'extras'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1581878742461-Curriculum_Joselyn_Mari_2020_1'"
     ]
    }
   ],
   "source": [
    "# Se crea un diccionario vacio para rellenarlo\n",
    "secciones_data = {\n",
    "    'extras' : '',\n",
    "    'experiencia' : '',\n",
    "    'educación' : '',\n",
    "    'skills':''\n",
    "                    \n",
    "        \n",
    "}\n",
    "\n",
    "\n",
    "# Se carga un archivo .txt, que contiene el CV que venia del PDF\n",
    "\n",
    "#file = \"resumes_text_output/1580573732905-DiegoAlbertoGutierrezBejarano\"\n",
    "#file = 'resumes_text_output/prueba_genoma_experiencia'\n",
    "file = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1581878742461-Curriculum_Joselyn_Mari_2020_1'\n",
    "cv_txt = open(file, \"r\")\n",
    "previous_section  = 'extras'\n",
    "\n",
    "\n",
    "\n",
    "for line in cv_txt:\n",
    "    # si la linea esta vacia, entonces saltar\n",
    "    if (len(line.strip()) == 0 or is_empty(line)):\n",
    "        continue\n",
    "\n",
    "    # procesar la siguiente linea\n",
    "    list_of_words_in_line = re_c.findall(line)\n",
    "    list_of_imp_words_in_line  = []\n",
    "    \n",
    "    # recorrer todas las palabras de linea actual\n",
    "    for i in range(len(list_of_words_in_line)):\n",
    "        modified_word = modify(list_of_words_in_line[i])\n",
    "\n",
    "        if (modified_word): \n",
    "            list_of_imp_words_in_line.append(modified_word)\n",
    "\n",
    "    curr_line = ' '.join(list_of_imp_words_in_line)\n",
    "    doc = nlp(curr_line)\n",
    "    #print(doc)\n",
    "    section_value = {}\n",
    "\n",
    "    # initializing section values to zero\n",
    "    for section in list_of_sections:\n",
    "        section_value[section] = 0.0\n",
    "    section_value[None] = 0.0\n",
    "\n",
    "    # updating section values    \n",
    "    for token in doc:\n",
    "        for section in list_of_sections:\n",
    "            for word in similar_to[section]:\n",
    "                #word_token = doc.vocab[word]\n",
    "                try:\n",
    "                    section_value[section] = max(section_value[section], float(model.similarity(token.text, word)))\n",
    "                except: \n",
    "                    pass # si es que token.text no esta en el vocabulario\n",
    "                \n",
    "    # ver la siguiente sección de acuerdo al umbral establecido\n",
    "    most_likely_section = None\n",
    "    for section in list_of_sections:\n",
    "        if (section_value[most_likely_section] < section_value[section] and section_value[section] > threshold):\n",
    "            most_likely_section = section\n",
    "\n",
    "    # updating the section\n",
    "    if (previous_section != most_likely_section and most_likely_section is not None):\n",
    "        previous_section = most_likely_section\n",
    "\n",
    "\n",
    "    # writing data to the pandas series\n",
    "    try:\n",
    "        docx = nlp(line)\n",
    "    except:\n",
    "        continue  # si que hay simbolos raros\n",
    "    mod_line = ''\n",
    "    for token in docx:\n",
    "        if (not token.is_stop):\n",
    "            mod_line += token.lemma_ + ' '\n",
    "\n",
    "    secciones_data[previous_section] += mod_line\n",
    "\n",
    "\n",
    "cv_txt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = file.replace(\"resumes_text_output/\", '').replace('.txt', '')\n",
    "\n",
    "with open('output_seccionizado/'+name+'.json', 'w',encoding='utf-8') as json_file:\n",
    "    json.dump(secciones_data, json_file,ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
