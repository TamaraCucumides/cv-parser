{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import es_core_news_sm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import es_core_news_md\n",
    "import itertools\n",
    "from nltk.stem import SnowballStemmer\n",
    "import textacy\n",
    "import regex\n",
    "import unidecode\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import re\n",
    "import json\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "re_c = re.compile(r'\\w+')\n",
    "wordvectors_file_vec ='/home/erwin/Genoma/cv-parser/parser/embeddings/fasttext-sbwc.3.6.e20.vec'\n",
    "nlp = es_core_news_md.load()\n",
    "cantidad = 500000\n",
    "\n",
    "#model = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= '/home/erwin/Genoma/cv-parser/parser/resumes_pdf/prueba_genoma_experiencia.pdf'\n",
    "#file ='/home/erwin/Genoma/cv-parser/resumes/1574824594718-Ing._Comercial_Paula_Fetis_CV.pdf'\n",
    "#file ='/home/erwin/Genoma/cv-parser/resumes/1580248761709-CV_Sergio_Soto_Valdes.pdf'\n",
    "file = '/home/erwin/Genoma/cv-parser/parser/resumes_pdf/1582718980347-CV_-_Mario_Espinoza1.pdf'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mario Sergio Espinoza Acero\n",
      "981999616 / 346-1658\n",
      "Dni 45629118\n",
      "Mariosergioespinoza89@gmail.com\n",
      "Titulado en Ciencias de la Comunicación por la Universidad de Lima, con especialidad en\n",
      "Marketing y Gestión Comercial.\n",
      "Persona proactiva, ordenada y con alto sentido de la responsabilidad. Considera que el trabajo\n",
      "en equipo es parte fundamental de una empresa, siempre orientado a resultados tangibles.\n",
      "Actualmente llevando el programa de Mba Candidate por la Universidad de Lima, promoción\n",
      "2018-2\n",
      "Experiencia Laboral\n",
      "Adecco Peru\n",
      "Cargo Gerente de Desarrollo\n",
      "Fecha 01/04/2019 a la fecha.\n",
      "Personal a cargo 10 colaboradores.\n",
      "Funciones\n",
      "Responsable del manejo de la cartera Top de clientes.\n",
      "Análisis periódico del estado de gestión e indicadores de la sucursal. Margen Bruto por\n",
      "Servicio, General, Presupuesto de Gastos de Operaciones y Gastos de personal\n",
      "Responsable del cumplimiento de los objetivos de ventas por línea de negocio, garantizando\n",
      "su rentabilidad y nivel de servicio\n",
      "Desarrollo de convenios con diversas instituciones.\n",
      "Implementación de canales de post-venta para clientes extranjeros\n",
      "Enfoque de negocio en clientes extranjeros y desarrollo de nuevos servicios a la medida del\n",
      "cliente.\n",
      "Adecco Peru\n",
      "Cargo Gerente de Sucursal.\n",
      "Fecha 01/01/2017 a 31/03/2019\n",
      "Personal a cargo 10 colaboradores.\n",
      "Funciones\n",
      "Responsable de la gestión integral de la Sucursal\n",
      "Análisis periódico del estado de gestión e indicadores de la sucursal. Margen Bruto por\n",
      "Servicio, General, Presupuesto de Gastos de Operaciones y Gastos de personal\n",
      "Responsable del cumplimiento de los objetivos de ventas por línea de negocio, garantizando\n",
      "su rentabilidad y nivel de servicio\n",
      "Desarrollo y ejecución de planes de negocios y venta, KPIs y estrategias comerciales\n",
      "orientadas a la captación, mantenimiento y retención de clientes\n",
      "Presentación y venta de los servicios de Selección, Outsourcing, Intermediación, Payroll,\n",
      "Consultoría y Formación\n",
      "Adecco Peru\n",
      "Cargo Ejecutivo Comercial.\n",
      "Fecha 25/03/2015 hasta 31/12/2016\n",
      "Funciones\n",
      "Ejecutar estrategias comerciales orientadas a la captación, mantenimiento y retención de\n",
      "clientes de la sucursal.\n",
      "Identificar información de negocios relevantes, así como nuevas oportunidades comerciales.\n",
      "Realizar el seguimiento de las propuestas comerciales presentadas a clientes.\n",
      "Mantener comunicación estable y periódica con los clientes realizando visitas de\n",
      "mantenimiento.\n",
      "Hacer seguimiento a la gestión de facturación y cobranzas de los clientes.\n",
      "Elaborar propuestas comerciales acorde al servicio.\n",
      "Logros Premio Latam a mejor Gestión Comercial 2015.\n",
      "Freno S.a.\n",
      "Cargo Coordinador de Inteligencia Comercial\n",
      "Fecha 01/08/2014 hasta 20/03/2015\n",
      "Personal a Cargo 7 personas\n",
      "Funciones\n",
      "Supervisión de Fuerza de Ventas en Lima.\n",
      "Estudio de mercado para el lanzamiento de nuevos productos y mejorar el portafolio de\n",
      "productos.\n",
      "Servicio de Post-Venta a distribuidor minorista.\n",
      "Prospección de nuevos puntos de venta en Lima Metropolitana.\n",
      "Bolsa de Valores de Lima\n",
      "Cargo Ejecutivo Comercial\n",
      "Fecha 01/12/2012 hasta 31/07/2014\n",
      "Ejecutar estrategias comerciales orientadas a la captación, mantenimiento y retención de\n",
      "nuevos clientes.\n",
      "Identificar información de negocios relevantes, así como nuevas oportunidades comerciales.\n",
      " Enfoque en la venta de los diversos cursos que dicta el Centro de Estudios Bursátiles-Bursen.\n",
      "Elaborar propuestas comerciales acorde al servicio.\n",
      "Logros Meta Anual del 2013 superada en un 30\n",
      "Formación\n",
      "\n",
      "Universidad Universidad de Lima.\n",
      "\n",
      "Grado Obtenido Titulado.\n",
      "\n",
      "Profesión Ciencias de la Comunicación.\n",
      "Estudios Post  Grado\n",
      "\n",
      "Universidad Esan.\n",
      "\n",
      "Grado Obtenido Certificación Internacional.\n",
      "\n",
      "Especialización Diplomado especializado en Marketing.\n",
      "Otros Estudios\n",
      "\n",
      "Idiomas Ingles\n",
      "\n",
      "Nivel Avanzado\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_text(path):\n",
    "    '''\n",
    "    Input: ruta hacia los archivos\n",
    "    Salida: Texto plano como string\n",
    "    Primero se extrae el texto usando fitz (PyMUPDF)\n",
    "    Luego, debido a que la salida no es perfecta, se limpia\n",
    "    eliminando todos los simbolos innecesarios.\n",
    "    Además para efectos de deteccion de nombres, \n",
    "    palabras completamente en mayusculas se capitalizan\n",
    "    sólo al principio. PAULA ---> Paula.\n",
    "    Esto hace más robusto la detección de entidades.\n",
    "    '''\n",
    "    with fitz.open(path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc: # text contiene todo el texto extraido desde el PDF\n",
    "            text += page.getText()\n",
    "        \n",
    "        text_2 = '' #texto sin mayusculas y saltos innecesarias \n",
    "        for line in text.splitlines():\n",
    "            if not line.strip(): #si la linea esta vacia, saltar\n",
    "                continue\n",
    "            line_2=''\n",
    "            for word in line.split():\n",
    "                if word.isupper(): # Si la palabra esta completamente en mayuscula\n",
    "                    line_2 += word.capitalize()+' '\n",
    "\n",
    "                else:\n",
    "                    line_2 += word+ ' '\n",
    "            text_2 += \" \".join(line_2.split()) +'\\n'\n",
    "        \n",
    "        simbolos = ' -,\\n./@' #Simbolos que se permiten, sirven para correo, links, etc.\n",
    "        text_clean = ' '\n",
    "        for char in text_2:\n",
    "            if (char.isalnum())| (char in simbolos): #Si el char es alphanumerico o es un simbolo permitido\n",
    "                text_clean += char\n",
    "       \n",
    "        text = text_clean\n",
    "    return text\n",
    "\n",
    "text = extract_text(file)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dni\n"
     ]
    }
   ],
   "source": [
    "nlp = es_core_news_sm.load()\n",
    "nlp_t = nlp(text)\n",
    "\n",
    "noun_chunks = list(nlp_t.noun_chunks)\n",
    "print(noun_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan los nombres de las secciones, se clasifican en \n",
    "# Perfil, Experiencia, Formación académica y skills\n",
    "\n",
    "seccion_csv = '/home/erwin/Genoma/cv-parser/parser/CSVs/seccionesCV.csv'\n",
    "secciones = pd.read_csv(seccion_csv, header = 0)\n",
    "secciones.columns = secciones.loc[0] \n",
    "#secciones.columns\n",
    "\n",
    "\n",
    "secciones_dict = {\n",
    "    'extras' : [str(x.lower()) for x in secciones.Perfil.values if str(x)!= 'nan'],\n",
    "    'experiencia' : [str(x.lower()) for x in secciones['Experiencia '].values if str(x)!= 'nan'],\n",
    "    'educación' : [str(x.lower()) for x in secciones['Formación Académica'].values if str(x)!= 'nan'],\n",
    "    'skills' : [str(x.lower()) for x in secciones['skills'].values if str(x)!= 'nan']                   \n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch for debug\n",
    "flag_print = False\n",
    "\n",
    "# switch to clear existing data\n",
    "flag_clear = True\n",
    "\n",
    "#threshold value for determining section\n",
    "threshold = 0.45\n",
    "\n",
    "similar_to = secciones_dict\n",
    "\n",
    "\n",
    "list_of_sections = similar_to.keys()\n",
    "\n",
    "# Usando secciones_dict que tiene las secciones a buscar y palabras que describen esas secciones\n",
    "# se llevan aquellas palabras a su lema\n",
    "for section in list_of_sections:\n",
    "    new_list = []\n",
    "    \n",
    "    for word in similar_to[section]:\n",
    "        docx = nlp(word)\n",
    "        new_list.append(docx[0].lemma_)\n",
    "    \n",
    "        \n",
    "    similar_to[section] = list(set(new_list)) # se retorna lista de elementos unicos\n",
    "#pp.pprint(similar_to)\n",
    "\n",
    "\n",
    "# function to remove unnecessary symbols and stopwords \n",
    "def modify(word):\n",
    "    try:\n",
    "        symbols = '''~'`!@#$%^&*)(_+-=}{][|\\:;\",./<>?'''\n",
    "        mod_word = ''\n",
    "        \n",
    "        for char in word:\n",
    "            if (char not in symbols):\n",
    "                mod_word += char.lower()\n",
    "\n",
    "        docx = nlp(mod_word)\n",
    "\n",
    "        if (len(mod_word) == 0 or docx[0].is_stop):\n",
    "            return None\n",
    "        else:\n",
    "            return docx[0].lemma_\n",
    "    except:\n",
    "        return None # to handle the odd case of characters like 'x02', etc.\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def is_empty(line):\n",
    "    '''\n",
    "    Retorna un booleano correspondiendo a \n",
    "    si una linea esta vacia en términos de letras-números\n",
    "    '''\n",
    "    for c in line:\n",
    "        if (c.isalpha()):\n",
    "            return False\n",
    "    return True\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un diccionario vacio para rellenarlo\n",
    "secciones_data = {\n",
    "    'extras' : '',\n",
    "    'experiencia' : '',\n",
    "    'educación' : '',\n",
    "    'skills':''\n",
    "                    \n",
    "        \n",
    "}\n",
    "\n",
    "\n",
    "# Se carga un archivo .txt, que contiene el CV que venia del PDF\n",
    "\n",
    "#file = \"resumes_text_output/1580573732905-DiegoAlbertoGutierrezBejarano\"\n",
    "#file = 'resumes_text_output/prueba_genoma_experiencia'\n",
    "file = '/home/erwin/Genoma/cv-parser/parser/Outputs/output_text/1581878742461-Curriculum_Joselyn_Mari_2020_1'\n",
    "cv_txt = open(file, \"r\")\n",
    "previous_section  = 'extras'\n",
    "\n",
    "\n",
    "\n",
    "for line in cv_txt:\n",
    "    # si la linea esta vacia, entonces saltar\n",
    "    if (len(line.strip()) == 0 or is_empty(line)):\n",
    "        continue\n",
    "\n",
    "    # procesar la siguiente linea\n",
    "    list_of_words_in_line = re_c.findall(line)\n",
    "    list_of_imp_words_in_line  = []\n",
    "    \n",
    "    # recorrer todas las palabras de linea actual\n",
    "    for i in range(len(list_of_words_in_line)):\n",
    "        modified_word = modify(list_of_words_in_line[i])\n",
    "\n",
    "        if (modified_word): \n",
    "            list_of_imp_words_in_line.append(modified_word)\n",
    "\n",
    "    curr_line = ' '.join(list_of_imp_words_in_line)\n",
    "    doc = nlp(curr_line)\n",
    "    #print(doc)\n",
    "    section_value = {}\n",
    "\n",
    "    # initializing section values to zero\n",
    "    for section in list_of_sections:\n",
    "        section_value[section] = 0.0\n",
    "    section_value[None] = 0.0\n",
    "\n",
    "    # updating section values    \n",
    "    for token in doc:\n",
    "        for section in list_of_sections:\n",
    "            for word in similar_to[section]:\n",
    "                #word_token = doc.vocab[word]\n",
    "                try:\n",
    "                    section_value[section] = max(section_value[section], float(model.similarity(token.text, word)))\n",
    "                except: \n",
    "                    pass # si es que token.text no esta en el vocabulario\n",
    "                \n",
    "    # ver la siguiente sección de acuerdo al umbral establecido\n",
    "    most_likely_section = None\n",
    "    for section in list_of_sections:\n",
    "        if (section_value[most_likely_section] < section_value[section] and section_value[section] > threshold):\n",
    "            most_likely_section = section\n",
    "\n",
    "    # updating the section\n",
    "    if (previous_section != most_likely_section and most_likely_section is not None):\n",
    "        previous_section = most_likely_section\n",
    "\n",
    "\n",
    "    # writing data to the pandas series\n",
    "    try:\n",
    "        docx = nlp(line)\n",
    "    except:\n",
    "        continue  # si que hay simbolos raros\n",
    "    mod_line = ''\n",
    "    for token in docx:\n",
    "        if (not token.is_stop):\n",
    "            mod_line += token.lemma_ + ' '\n",
    "\n",
    "    secciones_data[previous_section] += mod_line\n",
    "\n",
    "\n",
    "cv_txt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = file.replace(\"resumes_text_output/\", '').replace('.txt', '')\n",
    "\n",
    "with open('output_seccionizado/'+name+'.json', 'w',encoding='utf-8') as json_file:\n",
    "    json.dump(secciones_data, json_file,ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
