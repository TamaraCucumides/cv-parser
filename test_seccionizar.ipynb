{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import es_core_news_sm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import es_core_news_md\n",
    "import itertools\n",
    "from nltk.stem import SnowballStemmer\n",
    "import textacy\n",
    "import regex\n",
    "import unidecode\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import re\n",
    "import json\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "re_c = re.compile(r'\\w+')\n",
    "wordvectors_file_vec ='/home/erwin/Genoma/cv-parser/fasttext-sbwc.3.6.e20.vec'\n",
    "nlp = es_core_news_md.load()\n",
    "cantidad = 100000\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= '/home/erwin/Genoma/cv-parser/resumes/prueba_genoma_experiencia.pdf'\n",
    "#file ='/home/erwin/Genoma/cv-parser/resumes/1574824594718-Ing._Comercial_Paula_Fetis_CV.pdf'\n",
    "#file ='/home/erwin/Genoma/cv-parser/resumes/1580248761709-CV_Sergio_Soto_Valdes.pdf'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(path):\n",
    "    '''\n",
    "    Input: ruta hacia los archivos\n",
    "    Salida: Texto plano como string\n",
    "    '''\n",
    "    with fitz.open(path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.getText()\n",
    "        # eliminar estos simbolos\n",
    "        \n",
    "        text_clean = text\n",
    "        text_2 = ''\n",
    "        for line in text_clean.splitlines():\n",
    "            if not line.strip(): #si la linea esta vacia, saltar\n",
    "                continue\n",
    "            line_2=''\n",
    "            for word in line.split():\n",
    "                if word.isupper():\n",
    "                    line_2 += word.capitalize()+' '\n",
    "\n",
    "                else:\n",
    "                    line_2 += word+ ' '\n",
    "            text_2 += line_2 +'\\n'\n",
    "        \n",
    "        simbolos = ' ,\\n./:@'\n",
    "        text_clean = ''\n",
    "        for char in text_2:\n",
    "            if (char.isalnum())| (char in simbolos):\n",
    "                text_clean += char\n",
    "        #print(text_clean)\n",
    "        # Limpiar palabras completamente en mayusculas, es importante\n",
    "        # para reconocer los nombres\n",
    "        \n",
    "        text = text_clean\n",
    "    return text\n",
    "\n",
    "\n",
    "text = extract_text(file)\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan los nombres de las secciones, se clasifican en \n",
    "# Perfil, Experiencia, Formación académica y skills\n",
    "\n",
    "seccion_csv = 'parser/seccionesCV.csv'\n",
    "secciones = pd.read_csv(seccion_csv, header = 0)\n",
    "secciones.columns = secciones.loc[0] \n",
    "#secciones.columns\n",
    "\n",
    "\n",
    "secciones_dict = {\n",
    "    'extras' : [str(x.lower()) for x in secciones.Perfil.values if str(x)!= 'nan'],\n",
    "    'experiencia' : [str(x.lower()) for x in secciones['Experiencia '].values if str(x)!= 'nan'],\n",
    "    'educación' : [str(x.lower()) for x in secciones['Formación Académica'].values if str(x)!= 'nan'],\n",
    "    'skills' : [str(x.lower()) for x in secciones['skills'].values if str(x)!= 'nan']                   \n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch for debug\n",
    "flag_print = False\n",
    "\n",
    "# switch to clear existing data\n",
    "flag_clear = True\n",
    "\n",
    "#threshold value for determining section\n",
    "threshold = 0.45\n",
    "\n",
    "similar_to = secciones_dict\n",
    "\n",
    "\n",
    "list_of_sections = similar_to.keys()\n",
    "\n",
    "# Usando secciones_dict que tiene las secciones a buscar y palabras que describen esas secciones\n",
    "# se llevan aquellas palabras a su lema\n",
    "for section in list_of_sections:\n",
    "    new_list = []\n",
    "    \n",
    "    for word in similar_to[section]:\n",
    "        docx = nlp(word)\n",
    "        new_list.append(docx[0].lemma_)\n",
    "    \n",
    "        \n",
    "    similar_to[section] = list(set(new_list)) # se retorna lista de elementos unicos\n",
    "#pp.pprint(similar_to)\n",
    "\n",
    "\n",
    "# function to remove unnecessary symbols and stopwords \n",
    "def modify(word):\n",
    "    try:\n",
    "        symbols = '''~'`!@#$%^&*)(_+-=}{][|\\:;\",./<>?'''\n",
    "        mod_word = ''\n",
    "        \n",
    "        for char in word:\n",
    "            if (char not in symbols):\n",
    "                mod_word += char.lower()\n",
    "\n",
    "        docx = nlp(mod_word)\n",
    "\n",
    "        if (len(mod_word) == 0 or docx[0].is_stop):\n",
    "            return None\n",
    "        else:\n",
    "            return docx[0].lemma_\n",
    "    except:\n",
    "        return None # to handle the odd case of characters like 'x02', etc.\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def is_empty(line):\n",
    "    '''\n",
    "    Retorna un booleano correspondiendo a \n",
    "    si una linea esta vacia en términos de letras-números\n",
    "    '''\n",
    "    for c in line:\n",
    "        if (c.isalpha()):\n",
    "            return False\n",
    "    return True\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un diccionario vacio para rellenarlo\n",
    "secciones_data = {\n",
    "    'extras' : '',\n",
    "    'experiencia' : '',\n",
    "    'educación' : '',\n",
    "    'skills':''\n",
    "                    \n",
    "        \n",
    "}\n",
    "\n",
    "\n",
    "# Se carga un archivo .txt, que contiene el CV que venia del PDF\n",
    "\n",
    "#file = \"resumes_text_output/1580573732905-DiegoAlbertoGutierrezBejarano\"\n",
    "#file = 'resumes_text_output/prueba_genoma_experiencia'\n",
    "file = 'resumes_text_output/1581878742461-Curriculum_Joselyn_Mari_2020_1'\n",
    "cv_txt = open(file, \"r\")\n",
    "previous_section  = 'extras'\n",
    "\n",
    "\n",
    "\n",
    "for line in cv_txt:\n",
    "    # si la linea esta vacia, entonces saltar\n",
    "    if (len(line.strip()) == 0 or is_empty(line)):\n",
    "        continue\n",
    "\n",
    "    # procesar la siguiente linea\n",
    "    list_of_words_in_line = re_c.findall(line)\n",
    "    list_of_imp_words_in_line  = []\n",
    "    \n",
    "    # recorrer todas las palabras de linea actual\n",
    "    for i in range(len(list_of_words_in_line)):\n",
    "        modified_word = modify(list_of_words_in_line[i])\n",
    "\n",
    "        if (modified_word): \n",
    "            list_of_imp_words_in_line.append(modified_word)\n",
    "\n",
    "    curr_line = ' '.join(list_of_imp_words_in_line)\n",
    "    doc = nlp(curr_line)\n",
    "    #print(doc)\n",
    "    section_value = {}\n",
    "\n",
    "    # initializing section values to zero\n",
    "    for section in list_of_sections:\n",
    "        section_value[section] = 0.0\n",
    "    section_value[None] = 0.0\n",
    "\n",
    "    # updating section values    \n",
    "    for token in doc:\n",
    "        for section in list_of_sections:\n",
    "            for word in similar_to[section]:\n",
    "                #word_token = doc.vocab[word]\n",
    "                try:\n",
    "                    section_value[section] = max(section_value[section], float(model.similarity(token.text, word)))\n",
    "                except: \n",
    "                    pass # si es que token.text no esta en el vocabulario\n",
    "                \n",
    "    # ver la siguiente sección de acuerdo al umbral establecido\n",
    "    most_likely_section = None\n",
    "    for section in list_of_sections:\n",
    "        if (section_value[most_likely_section] < section_value[section] and section_value[section] > threshold):\n",
    "            most_likely_section = section\n",
    "\n",
    "    # updating the section\n",
    "    if (previous_section != most_likely_section and most_likely_section is not None):\n",
    "        previous_section = most_likely_section\n",
    "\n",
    "\n",
    "    # writing data to the pandas series\n",
    "    try:\n",
    "        docx = nlp(line)\n",
    "    except:\n",
    "        continue  # si que hay simbolos raros\n",
    "    mod_line = ''\n",
    "    for token in docx:\n",
    "        if (not token.is_stop):\n",
    "            mod_line += token.lemma_ + ' '\n",
    "\n",
    "    secciones_data[previous_section] += mod_line\n",
    "\n",
    "\n",
    "cv_txt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = file.replace(\"resumes_text_output/\", '').replace('.txt', '')\n",
    "\n",
    "with open('output_seccionizado/'+name+'.json', 'w',encoding='utf-8') as json_file:\n",
    "    json.dump(secciones_data, json_file,ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cv_parser] *",
   "language": "python",
   "name": "conda-env-cv_parser-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
