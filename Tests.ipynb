{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import es_core_news_sm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import es_core_news_sm\n",
    "import itertools\n",
    "from nltk.stem import SnowballStemmer\n",
    "import textacy\n",
    "import regex\n",
    "import unidecode\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "wordvectors_file_vec ='/home/erwin/Genoma/cv-parser/fasttext-sbwc.3.6.e20.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantidad = 100000\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= '/home/erwin/Genoma/cv-parser/resumes/prueba_genoma_experiencia.pdf'\n",
    "#file ='/home/erwin/Genoma/cv-parser/resumes/1574824594718-Ing._Comercial_Paula_Fetis_CV.pdf'\n",
    "#file ='/home/erwin/Genoma/cv-parser/resumes/1580248761709-CV_Sergio_Soto_Valdes.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(path):\n",
    "    '''\n",
    "    Input: ruta hacia los archivos\n",
    "    Salida: Texto plano como string\n",
    "    '''\n",
    "    with fitz.open(path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.getText()\n",
    "        # eliminar estos simbolos\n",
    "        \n",
    "        text_clean = text\n",
    "        text_2 = ''\n",
    "        for line in text_clean.splitlines():\n",
    "            if not line.strip(): #si la linea esta vacia, saltar\n",
    "                continue\n",
    "            line_2=''\n",
    "            for word in line.split():\n",
    "                if word.isupper():\n",
    "                    line_2 += word.capitalize()+' '\n",
    "\n",
    "                else:\n",
    "                    line_2 += word+ ' '\n",
    "            text_2 += line_2 +'\\n'\n",
    "        \n",
    "        simbolos = ' ,\\n./:@'\n",
    "        text_clean = ''\n",
    "        for char in text_2:\n",
    "            if (char.isalnum())| (char in simbolos):\n",
    "                text_clean += char\n",
    "        #print(text_clean)\n",
    "        # Limpiar palabras completamente en mayusculas, es importante\n",
    "        # para reconocer los nombres\n",
    "        \n",
    "        text = text_clean\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curriculum Vitae \n",
      "Erwin Nicolás Paillacán Huaitro \n",
      "Enero 7, 1997 \n",
      "erwinpaillacan@gmail.com \n",
      " \n",
      "569 86634232 \n",
      " \n",
      "Gran Avenida 3806 \n",
      " \n",
      "Región Metropolitana \n",
      "https://www.linkedin.com/in/erwinpaillacan/ \n",
      "Descripción \n",
      "Oriundo de la ciudad de Osorno, actualmente curso el \n",
      "sexto año de Ingeniería Civil Eléctrica con énfasis en \n",
      "Telecomunicaciones e Inteligencia Computacional. Me \n",
      "gusta programar, mirar series y aprender cosas intere \n",
      "santes en Coursera. Aparte de ser un apasionado por \n",
      "el mundo teleco, me gusta mucho el mundo de los da \n",
      "tos y el procesamiento de imágenes. En esto último ten \n",
      "go algunos proyectos para la Universidad en mi GitHub \n",
      "https: // github. com/ erwinpaillacan . Fecha de ti \n",
      "tulación estimada 20211. \n",
      "Antecedentes Personales \n",
      "Fecha de Nacimiento: \n",
      "7 de Enero del 1997 \n",
      "Edad: \n",
      "23 años \n",
      "Nacionalidad: \n",
      "Chileno \n",
      "Cédula de Identidad: \n",
      "19.270.6769 \n",
      "Experiencia \n",
      "Telecsa \n",
      "Osorno, Región de los Lagos, Chile \n",
      "Alumno en Práctica I \n",
      "Enero 19  Febrero 19 \n",
      "Apoyo en área de transmisión y cooperados. 180 Horas \n",
      "Educación \n",
      "Universidad de Chile \n",
      "Santiago, Región Metropolitana, Chile \n",
      "Licenciatura en Ciencias de la Ingeniería, Mención Eléctrica \n",
      "2015  2018 \n",
      "Minor Computación \n",
      " \n",
      "Ingeniería Civil Eléctrica, Telecomunicaciones e Inteligencia Computacional \n",
      "2015  2021 \n",
      "Czech Technical University in Prague \n",
      "Prague, Czech Republic \n",
      "Exchange Semester 20201 \n",
      " \n",
      "Conocimientos  Habilidades \n",
      "Uso de software: Excel, Latex, Octave, Hfss, Ni Awr Design Environment, Ltspice, OMNeT, Git. \n",
      "Uso de Lenguaje Programación y Librerías. Nivel Básico: Python, R, Sql, Pytorch, TensorFlow, M, \n",
      "OpenCV, C, Verilog, Assembler, Java. \n",
      "Conociemientos generales: Docker, Aws, Kubernetes, Linux, Microservicios \n",
      "Idiomas: Español, Inglés \n",
      "Cursos Online Programa Especializado  Deep Learning 1, Natural Language Processing with Classi \n",
      "ﬁcation and Vector Spaces 2, Natural Language Processing with Probabilistic Models 3, Natural \n",
      "Language Processing with Sequence Models 4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = extract_text(file)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    '''Generate Vectora for sentences.'''\n",
    "    M = []\n",
    "    for w in s.split():\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v/np.sqrt((v**2).sum())\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    '''Return Cosine Similarity.'''\n",
    "    return  np.dot(vec1,vec2)/(np.linalg.norm(vec1)* np.linalg.norm(vec2))\n",
    "\n",
    "def get_closest(word, n):\n",
    "    '''Get n most similar words by words.'''\n",
    "    #This function can easily be expanded to get similar words to phrases--\n",
    "    #using sent2vec() method defined in WithWord2Vec notebook. \n",
    "    word = word.lower()\n",
    "    words = [word]\n",
    "    similar_vals = [1]\n",
    "    try:\n",
    "        similar_list = model.most_similar(positive=[word],topn=n)\n",
    "        \n",
    "        for tupl in similar_list:\n",
    "            words.append(tupl[0])\n",
    "            similar_vals.append(tupl[1])\n",
    "    except:\n",
    "        #If word not in vocabulary return same word and 1 similarity-- \n",
    "        #see initialisation of words, similarities.\n",
    "        pass\n",
    "    \n",
    "    return words, similar_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8897964\n"
     ]
    }
   ],
   "source": [
    "frase_1 = 'software developer'\n",
    "frase_2 = 'web developer'\n",
    "\n",
    "\n",
    "vector_sentence_1 = sent2vec(frase_1)\n",
    "vector_sentence_2 = sent2vec(frase_2)\n",
    "similitud = cosine_sim(vector_sentence_1,vector_sentence_2)\n",
    "\n",
    "print(similitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('experiencias', 0.7211438417434692),\n",
       " ('pericia', 0.6080396175384521),\n",
       " ('aprendidas', 0.5985227227210999),\n",
       " ('vivencia', 0.595109224319458),\n",
       " ('conocimientos', 0.5840999484062195),\n",
       " ('enriquecedora', 0.5687903761863708),\n",
       " ('adquirida', 0.5659759044647217),\n",
       " ('inexperiencia', 0.5602041482925415),\n",
       " ('aprendido', 0.5534920692443848),\n",
       " ('vivida', 0.5521409511566162)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['experiencia'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_description ='''ventas marketing excel'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ventas 1\n",
      "------------------------------------------------\n",
      "minoristas 0.6644167900085449\n",
      "------------------------------------------------\n",
      "vendidas 0.6640223264694214\n",
      "------------------------------------------------\n",
      "marketing 1\n",
      "------------------------------------------------\n",
      "márketing 0.8219829797744751\n",
      "------------------------------------------------\n",
      "mercadotecnia 0.7757836580276489\n",
      "------------------------------------------------\n",
      "excel 1\n",
      "------------------------------------------------\n",
      "powerpoint 0.6345570087432861\n",
      "------------------------------------------------\n",
      "word 0.6285042762756348\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/prateekguptaiiitk/Resume_Filtering/blob/develop/Scoring/CV_ranking.ipynb\n",
    "word_value = {}\n",
    "similar_words_needed = 2\n",
    "for word in prc_description.split():\n",
    "    similar_words, similarity = get_closest(word, similar_words_needed)\n",
    "    for i in range(len(similar_words)):\n",
    "        word_value[similar_words[i]] = word_value.get(similar_words[i], 0)+similarity[i]\n",
    "        print(similar_words[i], word_value[similar_words[i]])\n",
    "        print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar los los skills de los csv\n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "path_to_json = 'resumes_output/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "#print(json_files)  # for me this prints ['foo.json']\n",
    "jsons = []\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js)) as json_file:\n",
    "        jsons.append(json.load(json_file))\n",
    "        #print(json_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ventas', 'minoristas', 'vendidas', 'marketing', 'márketing', 'mercadotecnia', 'excel', 'powerpoint', 'word'])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_value.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marketing', 'ventas', 'excel', 'sap', 'office']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsons[0]['Skills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ventas\n",
      "marketing\n",
      "excel\n"
     ]
    }
   ],
   "source": [
    "count = {}\n",
    "for word in word_value.keys(): \n",
    "    count[word] = 0\n",
    "    if word in jsons[i]['Skills']:\n",
    "        print(word)\n",
    "        count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ventas': 1,\n",
       " 'minoristas': 0,\n",
       " 'vendidas': 0,\n",
       " 'marketing': 1,\n",
       " 'márketing': 0,\n",
       " 'mercadotecnia': 0,\n",
       " 'excel': 1,\n",
       " 'powerpoint': 0,\n",
       " 'word': 0}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entre\n",
      "entre\n",
      "entre\n",
      "{'ventas': 1, 'minoristas': 1, 'vendidas': 1, 'marketing': 1, 'márketing': 1, 'mercadotecnia': 1, 'excel': 1, 'powerpoint': 1, 'word': 1}\n",
      "{'ventas': 0.0, 'minoristas': 0.0, 'vendidas': 0.0, 'marketing': 0.0, 'márketing': 0.0, 'mercadotecnia': 0.0, 'excel': 0.0, 'powerpoint': 0.0, 'word': 0.0}\n"
     ]
    }
   ],
   "source": [
    "### ahora veamos si resulta el ranking\n",
    "\n",
    "\n",
    "no_of_cv = len(jsons)\n",
    "\n",
    "count = {}\n",
    "idf = {}\n",
    "for word in word_value.keys():\n",
    "    count[word] = 0\n",
    "    for i in range(no_of_cv):\n",
    "        jsons[i]['Skills'] = [x.lower() for x in jsons[i]['Skills']]\n",
    "        try:\n",
    "            #if word in cvs.loc(0)['skill'][i].split() or word in cvs.loc(0)['exp'][i].split():\n",
    "            if word in jsons[i]['Skills']:\n",
    "                print('entre')\n",
    "                count[word] += 1\n",
    "        except:\n",
    "            pass\n",
    "    if (count[word] == 0):\n",
    "        count[word] = 1\n",
    "    idf[word] = math.log(no_of_cv/count[word])\n",
    "print(count)\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score = {}\n",
    "for i in range(no_of_cv):\n",
    "    score[i] = 0\n",
    "    try:\n",
    "        for word in word_value.keys():\n",
    "            tf = jsons[i]['Skills'].count(word) #+ cvs.loc(0)['exp'][i].split().count(word)\n",
    "            score[i] += word_value[word]*tf*idf[word]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = []\n",
    "for i in range(no_of_cv):\n",
    "    sorted_list.append((score[i], jsons[i]['Nombre archivo']))\n",
    "    \n",
    "sorted_list.sort(reverse = True)\n",
    "\n",
    "#for s, i in sorted_list:\n",
    "#    if list(cvs)[i] != '.DS_Store':\n",
    "#        print(list(cvs)[i], ':', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, '1580573732905-DiegoAlbertoGutierrezBejarano')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cv_parser] *",
   "language": "python",
   "name": "conda-env-cv_parser-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
